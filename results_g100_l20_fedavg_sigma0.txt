------------------------------
     Round     |g_bf|    |noise|     |g_af| |g_af-g_bf|     shrink      numer
 denom    |delta| train_loss  train_acc  test_loss   test_acc   local[s]  global[s
]   elpsd[s]      error
         0 7.8224e+00 8.9631e+00 1.1944e+01 8.9631e+00       1.00       1.00
 1.00       7.82       2.15       0.17       2.14       0.18       9.63       0.01
       9.88 4.4510e-03
         1 9.3902e+01 8.8750e+00 9.4253e+01 8.8750e+00       1.00       1.00
 1.00      93.90       3.79       0.16       3.77       0.15       9.42       0.00
      19.53 1.1317e-03
         2 1.0000e+02 9.0749e+00 1.0037e+02 9.0749e+00       1.00       1.00
 1.00     100.35       5.31       0.10       5.28       0.11       9.46       0.00
      29.22 9.5844e-04
         3 1.0000e+02 8.9352e+00 1.0037e+02 8.9352e+00       1.00       1.00
 1.00     100.21       7.27       0.10       7.23       0.10       9.48       0.00
      38.94 1.3472e-03
         4 1.0000e+02 9.0450e+00 1.0018e+02 9.0450e+00       1.00       1.00
 1.00     100.24      10.08       0.10      10.01       0.10       9.47       0.00
      48.69 1.6969e-03
^CTraceback (most recent call last):
  File "run.py", line 53, in <module>
    global_model = server.train_round(dataloaders)
  File "/home/local/ASURITE/xzhao181/FedStein/src/server.py", line 59, in train_ro
und
    pseudo_grad, delta_c_i = self.clients[i].train_with_clipping(self.global_model
, dataloaders[i])
  File "/home/local/ASURITE/xzhao181/FedStein/src/client.py", line 119, in train_w
ith_clipping
    store_metrics(self.args, pseudo_grad, noisy_grad, noise)
  File "/home/local/ASURITE/xzhao181/FedStein/src/utils.py", line 136, in store_me
trics
    args.var_epsilon = verification_assumption(args, agg_clients_pseudo_grad_bf)
  File "/home/local/ASURITE/xzhao181/FedStein/src/utils.py", line 107, in verifica
tion_assumption
    A_samples = np.random.normal(loc=a, scale=args.sigma, size=(10000, args.d))
KeyboardInterrupt

(racr) xzhao181@en4228312l:~/FedStein$ python run.py --seed=1 --partition=non_iid
--n_local_epochs=1 --is_scaffold
time for reading data =  17.20815372467041
Model dimension d=7960
step size is  0.5
noise is  0.1
aggregation_method is  dp_fedavg
model is  mlp
------------------------------
     Round     |g_bf|    |noise|     |g_af| |g_af-g_bf|     shrink      numer
 denom    |delta| train_loss  train_acc  test_loss   test_acc   local[s]  global[s
]   elpsd[s]      error
         0 7.8224e+00 8.9631e+00 1.1944e+01 8.9631e+00       1.00       1.00
 1.00       7.82       2.15       0.17       2.14       0.18       9.62       0.01
       9.87 4.4510e-03
         1 9.3902e+01 8.8750e+00 9.4253e+01 8.8750e+00       1.00       1.00
 1.00      93.90       3.79       0.16       3.77       0.15       9.57       0.00
      19.68 1.1317e-03
         2 1.0000e+02 9.0749e+00 1.0037e+02 9.0749e+00       1.00       1.00
 1.00     100.35       5.31       0.10       5.28       0.11       9.58       0.00
      29.48 9.5844e-04
         3 1.0000e+02 8.9352e+00 1.0037e+02 8.9352e+00       1.00       1.00
 1.00     100.21       7.27       0.10       7.23       0.10       9.50       0.00
      39.23 1.3472e-03
^CTraceback (most recent call last):
  File "run.py", line 53, in <module>
    global_model = server.train_round(dataloaders)
  File "/home/local/ASURITE/xzhao181/FedStein/src/server.py", line 59, in train_ro
und
    pseudo_grad, delta_c_i = self.clients[i].train_with_clipping(self.global_model
, dataloaders[i])
  File "/home/local/ASURITE/xzhao181/FedStein/src/client.py", line 71, in train_wi
th_clipping
    loss = self.args.criterion(self.model(x), y)
  File "/home/local/ASURITE/xzhao181/anaconda3/envs/racr/lib/python3.8/site-packag
es/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/local/ASURITE/xzhao181/anaconda3/envs/racr/lib/python3.8/site-packag
es/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/ASURITE/xzhao181/FedStein/models/mlp.py", line 18, in forward
    return self.net(x)
  File "/home/local/ASURITE/xzhao181/anaconda3/envs/racr/lib/python3.8/site-packag
es/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/local/ASURITE/xzhao181/anaconda3/envs/racr/lib/python3.8/site-packag
es/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/ASURITE/xzhao181/anaconda3/envs/racr/lib/python3.8/site-packag
es/torch/nn/modules/container.py", line 219, in forward
    input = module(input)
  File "/home/local/ASURITE/xzhao181/anaconda3/envs/racr/lib/python3.8/site-packag
es/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/local/ASURITE/xzhao181/anaconda3/envs/racr/lib/python3.8/site-packag
es/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/local/ASURITE/xzhao181/anaconda3/envs/racr/lib/python3.8/site-packag
es/torch/nn/modules/flatten.py", line 50, in forward
    return input.flatten(self.start_dim, self.end_dim)
KeyboardInterrupt

(racr) xzhao181@en4228312l:~/FedStein
(racr) xzhao181@en4228312l:~/FedStein$
(racr) xzhao181@en4228312l:~/FedStein$ clear
(racr) xzhao181@en4228312l:~/FedStein$ python run.py --seed=1 --partition=non_iid
--n_local_epochs=1 --is_scaffold
Traceback (most recent call last):
  File "run.py", line 9, in <module>
    from src.server import get_aggregator
  File "/home/local/ASURITE/xzhao181/FedStein/src/__init__.py", line 2, in <module
>
    from .server import *
  File "/home/local/ASURITE/xzhao181/FedStein/src/server.py", line 5, in <module>
    from .data_utils import load_partitioned_dataset
ModuleNotFoundError: No module named 'src.data_utils'
(racr) xzhao181@en4228312l:~/FedStein$ python run.py --seed=1 --partition=non_iid
--n_local_epochs=1 --is_scaffold
time for reading data =  17.22216486930847
Model: mlp, Aggregation: dp_fedavg, Scaffold: True
LR_local: 0.5, LR_global: 1.0, Noise: 0.1, Clip: 100.0
------------------------------
     Round     |g_bf|    |noise|     |g_af| |g_af-g_bf|     shrink      numer
 denom    |delta| train_loss  train_acc  test_loss   test_acc   local[s]  global[s
]   elpsd[s]      error
Traceback (most recent call last):
  File "run.py", line 51, in <module>
    num_to_select = int(args.n_clients * args.client_fraction)
AttributeError: 'Namespace' object has no attribute 'client_fraction'
(racr) xzhao181@en4228312l:~/FedStein$ python run.py --seed=1 --partition=non_iid
--n_local_epochs=1 --is_scaffold
time for reading data =  17.326695680618286
Model: mlp, Aggregation: dp_fedavg, Scaffold: True
LR_local: 0.5, LR_global: 1.0, Noise: 0.1, Clip: 100.0
------------------------------
     Round     |g_bf|    |noise|     |g_af| |g_af-g_bf|     shrink      numer
 denom    |delta| train_loss  train_acc  test_loss   test_acc   local[s]  global[s
]   elpsd[s]      error
Traceback (most recent call last):
  File "run.py", line 58, in <module>
    log_contents(args)
  File "/home/local/ASURITE/xzhao181/FedStein/src/utils.py", line 68, in log_conte
nts
    args.agg_client_states_norm_bf,
AttributeError: 'Namespace' object has no attribute 'agg_client_states_norm_bf'
(racr) xzhao181@en4228312l:~/FedStein$ python run.py --seed=1 --partition=non_iid
--n_local_epochs=1 --is_scaffold
time for reading data =  17.126920223236084
Model: mlp, Aggregation: dp_fedavg, Scaffold: True
LR_local: 0.5, LR_global: 1.0, Noise: 0.1, Clip: 100.0
------------------------------
     Round     |g_bf|    |noise|     |g_af| |g_af-g_bf|     shrink      numer
 denom    |delta| train_loss  train_acc  test_loss   test_acc   local[s]  global[s
]   elpsd[s]      error
Traceback (most recent call last):
  File "run.py", line 50, in <module>
    global_model = server.train_round(client_dataloaders)
  File "/home/local/ASURITE/xzhao181/FedStein/src/server.py", line 36, in train_ro
und
    return self.aggregate(pseudo_grads, delta_c_s)
  File "/home/local/ASURITE/xzhao181/FedStein/src/server.py", line 55, in aggregat
e
    store_metrics(self.args, flat_agg_pseudo_grad_bf, flat_agg_pseudo_grad_af, noi
se)
  File "/home/local/ASURITE/xzhao181/FedStein/src/utils.py", line 136, in store_me
trics
    args.var_epsilon = verification_assumption(args, agg_clients_pseudo_grad_bf)
  File "/home/local/ASURITE/xzhao181/FedStein/src/utils.py", line 107, in verifica
tion_assumption
    A_samples = np.random.normal(loc=a, scale=args.sigma, size=(10000, args.d))
AttributeError: 'Namespace' object has no attribute 'd'
(racr) xzhao181@en4228312l:~/FedStein$ python run.py --seed=1 --partition=non_iid
--n_local_epochs=1 --is_scaffold
time for reading data =  17.23119020462036
Model dimension d=7960
step size is  0.5
noise is  0.1
aggregation_method is  dp_fedavg
model is  mlp
------------------------------
     Round     |g_bf|    |noise|     |g_af| |g_af-g_bf|     shrink      numer
 denom    |delta| train_loss  train_acc  test_loss   test_acc   local[s]  global[s
]   elpsd[s]      error
         0 0.0000e+00 8.9631e+00 8.9631e+00 8.9631e+00       1.00       1.00
 1.00       0.00       2.35       0.09       2.35       0.09       9.54       0.01
       9.79 4.3643e-05
         1 0.0000e+00 8.8750e+00 8.8750e+00 8.8750e+00       1.00       1.00
 1.00       0.00       2.39       0.07       2.39       0.07       9.36       0.00
      19.37 4.8210e-05
^CTraceback (most recent call last):
  File "run.py", line 53, in <module>
    global_model = server.train_round(dataloaders)
  File "/home/local/ASURITE/xzhao181/FedStein/src/server.py", line 59, in train_ro
und
    pseudo_grad, delta_c_i = self.clients[i].train_with_clipping(self.global_model
, dataloaders[i])
  File "/home/local/ASURITE/xzhao181/FedStein/src/client.py", line 80, in train_wi
th_clipping
    store_metrics(self.args, pseudo_grad, noisy_grad, noise)
  File "/home/local/ASURITE/xzhao181/FedStein/src/utils.py", line 136, in store_me
trics
    args.var_epsilon = verification_assumption(args, agg_clients_pseudo_grad_bf)
  File "/home/local/ASURITE/xzhao181/FedStein/src/utils.py", line 107, in verifica
tion_assumption
    A_samples = np.random.normal(loc=a, scale=args.sigma, size=(10000, args.d))
KeyboardInterrupt

(racr) xzhao181@en4228312l:~/FedStein$ clear

(racr) xzhao181@en4228312l:~/FedStein$ python run.py --seed=1 --partition=non_iid
--n_local_epochs=1 --is_scaffold
time for reading data =  17.305363416671753
Model dimension d=7960
step size is  0.5
noise is  0.1
aggregation_method is  dp_fedavg
model is  mlp
------------------------------
     Round     |g_bf|    |noise|     |g_af| |g_af-g_bf|     shrink      numer
 denom    |delta| train_loss  train_acc  test_loss   test_acc   local[s]  global[s
]   elpsd[s]      error
         0 8.4037e+00 8.9631e+00 1.2281e+01 8.9631e+00       1.00       1.00
 1.00       8.40       2.12       0.19       2.12       0.20       9.62       0.01
       9.87 5.1936e-03
         1 1.0000e+02 8.8750e+00 1.0042e+02 8.8750e+00       1.00       1.00
 1.00     110.36      28.11       0.09      28.65       0.09       9.58       0.00
      19.68 4.1382e-04
         2 1.0000e+02 9.0749e+00 1.0028e+02 9.0749e+00       1.00       1.00
 1.00     213.07      21.06       0.09      21.39       0.09       9.65       0.00
      29.59 3.9783e-04
         3 1.0000e+02 8.9352e+00 1.0043e+02 8.9352e+00       1.00       1.00
 1.00     313.61      27.14       0.09      27.60       0.09       9.58       0.00
      39.41 3.9462e-04
         4 1.0000e+02 9.0450e+00 1.0036e+02 9.0450e+00       1.00       1.00
 1.00     413.74      30.24       0.09      30.82       0.09       9.70       0.00
      49.38 3.9525e-04
^CTraceback (most recent call last):
  File "run.py", line 53, in <module>
    global_model = server.train_round(dataloaders)
  File "/home/local/ASURITE/xzhao181/FedStein/src/server.py", line 69, in train_ro
und
    pseudo_grad, delta_c_i = self.clients[i].train_with_clipping(self.global_model
, dataloaders[i])
  File "/home/local/ASURITE/xzhao181/FedStein/src/client.py", line 86, in train_wi
th_clipping

  File "/home/local/ASURITE/xzhao181/FedStein/src/utils.py", line 136, in store_me
trics
    args.var_epsilon = verification_assumption(args, agg_clients_pseudo_grad_bf)
  File "/home/local/ASURITE/xzhao181/FedStein/src/utils.py", line 109, in verifica
tion_assumption
    fractions = A_samples / norms_squared[:, np.newaxis]     # broadcasting
KeyboardInterrupt

(racr) xzhao181@en4228312l:~/FedStein$ clear
(racr) xzhao181@en4228312l:~/FedStein$ conda deactivate
(base) xzhao181@en4228312l:~/FedStein$ conda activate scaffold
(scaffold) xzhao181@en4228312l:~/FedStein$ clear
(scaffold) xzhao181@en4228312l:~/FedStein$  python ./src/server/fedavg.py --datase
t mnist --clip_bound=100 --global_epochs=100 --local_epochs=20 --dp_sigma=0
python: can't open file '/home/local/ASURITE/xzhao181/FedStein/./src/server/fedavg
.py': [Errno 2] No such file or directory
(scaffold) xzhao181@en4228312l:~/FedStein$ cd ..
(scaffold) xzhao181@en4228312l:~$ cd D
Differential-Privacy-for-Heterogeneous-Federated-Learning-main/ DP-Scaffold/
DP_FedScaff_Stein/
(scaffold) xzhao181@en4228312l:~$ cd DP
DP_FedScaff_Stein/ DP-Scaffold/
(scaffold) xzhao181@en4228312l:~$ cd DP
DP_FedScaff_Stein/ DP-Scaffold/
(scaffold) xzhao181@en4228312l:~$ cd DP_FedScaff_Stein/
(scaffold) xzhao181@en4228312l:~/DP_FedScaff_Stein$ clear
(scaffold) xzhao181@en4228312l:~/DP_FedScaff_Stein$ python ./src/server/fedavg.py
--dataset mnist --clip_bound=100 --global_epochs=100 --local_epochs=20 --dp_sigma=
0
/home/local/ASURITE/xzhao181/DP_FedScaff_Stein/src/server/base.py:68: FutureWarnin
g: You are using `torch.load` with `weights_only=False` (the current default value
), which uses the default pickle module implicitly. It is possible to construct ma
licious pickle data which will execute arbitrary code during unpickling (See https
://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more deta
ils). In a future release, the default value for `weights_only` will be flipped to
 `True`. This limits the functions that could be executed during unpickling. Arbit
rary objects will no longer be allowed to be loaded via this mode unless they are
explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We
recommend you start setting `weights_only=True` for any use case where you don't h
ave full control of the loaded file. Please open an issue on GitHub for any issues
 related to this experimental feature.
  self.global_params_dict = torch.load(self.temp_dir / "global_model.pt")
Find existed global model...

Have run 5 epochs already.

Backbone: LeNet5(

  (net): Sequential(

    (0): Conv2d(1, 6, kernel_size=(5, 5), stride=(1, 1))

    (1): ReLU(inplace=True)

    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False
)
    (3): Conv2d(6, 16, kernel_size=(5, 5), stride=(1, 1))

    (4): ReLU(inplace=True)

    (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False
)
    (6): Flatten(start_dim=1, end_dim=-1)

    (7): Linear(in_features=256, out_features=120, bias=True)

    (8): ReLU(inplace=True)

    (9): Linear(in_features=120, out_features=84, bias=True)

    (10): ReLU(inplace=True)

    (11): Linear(in_features=84, out_features=10, bias=True)

  )

)

Arguments:

{

    'global_epochs': 100,

    'local_epochs': 20,

    'local_lr': 0.01,

    'verbose_gap': 5,

    'dataset': 'mnist',

    'batch_size': -1,

    'gpu': 1,

    'log': 0,

    'seed': 17,

    'client_num_per_round': 5,

    'save_period': 5,

    'dp_sigma': 0.0,

    'clip_bound': 100.0

}

============================== TRAINING ==============================

============================== ROUND: 0 ==============================

client [3]   loss: 2.1743 -> 2.1271    accuracy: 42.63% -> 42.63%

client [4]   loss: 3.2734 -> 3.1907    accuracy: 0.05% -> 0.05%

client [0]   loss: 2.5312 -> 2.4325    accuracy: 0.00% -> 0.00%

client [1]   loss: 2.4597 -> 2.3853    accuracy: 0.00% -> 0.00%

client [2]   loss: 2.5586 -> 2.4643    accuracy: 0.07% -> 0.07%

============================== ROUND: 5 ==============================

client [4]   loss: 3.1641 -> 3.0862    accuracy: 0.05% -> 0.05%

client [0]   loss: 2.4765 -> 2.3809    accuracy: 0.00% -> 0.00%

client [1]   loss: 2.3749 -> 2.2634    accuracy: 0.00% -> 0.00%

client [2]   loss: 2.4843 -> 2.3908    accuracy: 0.00% -> 0.07%

client [3]   loss: 2.1715 -> 2.1239    accuracy: 42.63% -> 42.63%

============================== ROUND: 10 ==============================

client [4]   loss: 3.0718 -> 2.9977    accuracy: 0.05% -> 0.05%

client [1]   loss: 2.3331 -> 2.2913    accuracy: 0.00% -> 0.00%

client [0]   loss: 2.4369 -> 2.3446    accuracy: 0.00% -> 0.00%

client [2]   loss: 2.4197 -> 2.3354    accuracy: 0.07% -> 0.00%

client [3]   loss: 2.1680 -> 2.1196    accuracy: 42.69% -> 42.69%

============================== ROUND: 15 ==============================

client [1]   loss: 2.3243 -> 2.2829    accuracy: 0.00% -> 0.00%

client [4]   loss: 2.9926 -> 2.9214    accuracy: 0.05% -> 0.05%

client [0]   loss: 2.4086 -> 2.3174    accuracy: 0.00% -> 0.00%

client [2]   loss: 2.3838 -> 2.2925    accuracy: 0.00% -> 0.00%

client [3]   loss: 2.1703 -> 2.1209    accuracy: 42.69% -> 42.69%

============================== ROUND: 20 ==============================

client [2]   loss: 2.3386 -> 2.2590    accuracy: 0.00% -> 0.00%

client [0]   loss: 2.3884 -> 2.2992    accuracy: 0.00% -> 0.00%

client [1]   loss: 2.3320 -> 2.2905    accuracy: 0.00% -> 0.00%

client [4]   loss: 2.9238 -> 2.8552    accuracy: 0.05% -> 0.05%

client [3]   loss: 2.1740 -> 2.1246    accuracy: 42.69% -> 42.63%

============================== ROUND: 25 ==============================

client [4]   loss: 2.8633 -> 2.7968    accuracy: 0.05% -> 0.05%

client [0]   loss: 2.3738 -> 2.2865    accuracy: 0.00% -> 0.00%

client [1]   loss: 2.3260 -> 2.2430    accuracy: 0.00% -> 0.00%

client [3]   loss: 2.1790 -> 2.1284    accuracy: 42.69% -> 42.69%

client [2]   loss: 2.3110 -> 2.2331    accuracy: 0.00% -> 0.00%

============================== ROUND: 30 ==============================

client [3]   loss: 2.1845 -> 2.1337    accuracy: 42.69% -> 42.69%

client [1]   loss: 2.3194 -> 2.2774    accuracy: 0.00% -> 0.00%

client [0]   loss: 2.3629 -> 2.2740    accuracy: 0.00% -> 0.00%

client [4]   loss: 2.8099 -> 2.7451    accuracy: 0.05% -> 0.05%

client [2]   loss: 2.2895 -> 2.2131    accuracy: 0.00% -> 0.00%

============================== ROUND: 35 ==============================

client [0]   loss: 2.3549 -> 2.2690    accuracy: 0.00% -> 0.00%

client [3]   loss: 2.1904 -> 2.1391    accuracy: 42.69% -> 42.69%

client [2]   loss: 2.2729 -> 2.1980    accuracy: 0.00% -> 0.00%

client [1]   loss: 2.3164 -> 2.2739    accuracy: 0.00% -> 44.11%

client [4]   loss: 2.7622 -> 2.6989    accuracy: 0.05% -> 0.05%

============================== ROUND: 40 ==============================

client [2]   loss: 2.2604 -> 2.1862    accuracy: 0.00% -> 0.00%

client [3]   loss: 2.1966 -> 2.1450    accuracy: 42.69% -> 42.69%

client [4]   loss: 2.7195 -> 2.6580    accuracy: 0.05% -> 0.05%

client [1]   loss: 2.3318 -> 2.2888    accuracy: 0.00% -> 44.11%

client [0]   loss: 2.3488 -> 2.2620    accuracy: 0.00% -> 0.00%

============================== ROUND: 45 ==============================

client [1]   loss: 2.3065 -> 2.2285    accuracy: 0.00% -> 44.11%

client [3]   loss: 2.2025 -> 2.1505    accuracy: 42.69% -> 42.69%

client [2]   loss: 2.2508 -> 2.1772    accuracy: 0.00% -> 0.00%

client [4]   loss: 2.6813 -> 2.6208    accuracy: 0.05% -> 0.05%

client [0]   loss: 2.3445 -> 2.2572    accuracy: 0.00% -> 0.00%

============================== ROUND: 50 ==============================

client [1]   loss: 2.2847 -> 2.2264    accuracy: 0.00% -> 44.11%

client [0]   loss: 2.3406 -> 2.2538    accuracy: 0.00% -> 0.00%

client [2]   loss: 2.2435 -> 2.1705    accuracy: 0.00% -> 0.00%

client [3]   loss: 2.2086 -> 2.1562    accuracy: 42.69% -> 42.69%

client [4]   loss: 2.6468 -> 2.5875    accuracy: 0.05% -> 0.05%

============================== ROUND: 55 ==============================

client [1]   loss: 2.2912 -> 2.2466    accuracy: 0.00% -> 44.11%

client [2]   loss: 2.2383 -> 2.1696    accuracy: 0.00% -> 0.00%

client [3]   loss: 2.2149 -> 2.1623    accuracy: 42.69% -> 42.69%

client [4]   loss: 2.6157 -> 2.5576    accuracy: 0.05% -> 0.05%

client [0]   loss: 2.3379 -> 2.2505    accuracy: 0.00% -> 0.00%

============================== ROUND: 60 ==============================

client [2]   loss: 2.2346 -> 2.1624    accuracy: 0.00% -> 0.00%

client [0]   loss: 2.3362 -> 2.2496    accuracy: 0.00% -> 70.08%

client [3]   loss: 2.2208 -> 2.1680    accuracy: 42.69% -> 42.69%

client [1]   loss: 2.2986 -> 2.2538    accuracy: 0.00% -> 44.11%

client [4]   loss: 2.5875 -> 2.5304    accuracy: 0.05% -> 0.05%

============================== ROUND: 65 ==============================

client [0]   loss: 2.3349 -> 2.2488    accuracy: 0.00% -> 70.08%

client [3]   loss: 2.2265 -> 2.1734    accuracy: 42.69% -> 42.69%

client [1]   loss: 2.2990 -> 2.2538    accuracy: 0.00% -> 44.11%

client [2]   loss: 2.2321 -> 2.1600    accuracy: 0.00% -> 0.00%

client [4]   loss: 2.5622 -> 2.5059    accuracy: 0.05% -> 0.05%

============================== ROUND: 70 ==============================

client [3]   loss: 2.2320 -> 2.1785    accuracy: 42.69% -> 42.69%

client [0]   loss: 2.3338 -> 2.2483    accuracy: 0.00% -> 70.08%

client [4]   loss: 2.5392 -> 2.4836    accuracy: 0.05% -> 0.05%

client [1]   loss: 2.2972 -> 2.2514    accuracy: 0.00% -> 44.11%

client [2]   loss: 2.2304 -> 2.1585    accuracy: 0.00% -> 0.00%

============================== ROUND: 75 ==============================

client [3]   loss: 2.2372 -> 2.1838    accuracy: 42.69% -> 42.69%

client [4]   loss: 2.5183 -> 2.4635    accuracy: 0.05% -> 0.05%

client [1]   loss: 2.3156 -> 2.2697    accuracy: 0.00% -> 44.11%

client [0]   loss: 2.3330 -> 2.2473    accuracy: 0.00% -> 70.08%

client [2]   loss: 2.2293 -> 2.1574    accuracy: 0.00% -> 55.35%

============================== ROUND: 80 ==============================

client [3]   loss: 2.2421 -> 2.1881    accuracy: 42.69% -> 42.69%

client [1]   loss: 2.3084 -> 2.2619    accuracy: 0.00% -> 44.11%

client [2]   loss: 2.2289 -> 2.1573    accuracy: 0.00% -> 55.35%

client [0]   loss: 2.3325 -> 2.2449    accuracy: 0.00% -> 70.08%

client [4]   loss: 2.4995 -> 2.4453    accuracy: 0.05% -> 0.05%

============================== ROUND: 85 ==============================

client [3]   loss: 2.2468 -> 2.1929    accuracy: 42.69% -> 42.69%

client [2]   loss: 2.2298 -> 2.1617    accuracy: 0.00% -> 55.35%

client [0]   loss: 2.3323 -> 2.2463    accuracy: 0.00% -> 70.08%

client [4]   loss: 2.4824 -> 2.4288    accuracy: 0.05% -> 0.05%

client [1]   loss: 2.3143 -> 2.2674    accuracy: 0.00% -> 44.11%

============================== ROUND: 90 ==============================

client [4]   loss: 2.4668 -> 2.4137    accuracy: 0.05% -> 0.05%

client [2]   loss: 2.2326 -> 2.1588    accuracy: 0.00% -> 55.35%

client [1]   loss: 2.3198 -> 2.2727    accuracy: 0.00% -> 44.11%

client [0]   loss: 2.3323 -> 2.2457    accuracy: 0.00% -> 70.08%

client [3]   loss: 2.2512 -> 2.1971    accuracy: 42.69% -> 42.69%

Training... ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 100% 0:02:58
============================== TESTING ==============================

Testing... ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 100% 0:00:01
==================== RESULTS ====================

loss: 2.3216    accuracy: 9.95%

(scaffold) xzhao181@en4228312l:~/DP_FedScaff_Stein$ python ./src/server/fedavg.py
--dataset mnist --clip_bound=100 --global_epochs=100 --local_epochs=20 --dp_sigma=
0
Backbone: LeNet5(

  (net): Sequential(

    (0): Conv2d(1, 6, kernel_size=(5, 5), stride=(1, 1))

    (1): ReLU(inplace=True)

    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False
)
    (3): Conv2d(6, 16, kernel_size=(5, 5), stride=(1, 1))

    (4): ReLU(inplace=True)

    (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False
)
    (6): Flatten(start_dim=1, end_dim=-1)

    (7): Linear(in_features=256, out_features=120, bias=True)

    (8): ReLU(inplace=True)

    (9): Linear(in_features=120, out_features=84, bias=True)

    (10): ReLU(inplace=True)

    (11): Linear(in_features=84, out_features=10, bias=True)

  )

)

Arguments:

{

    'global_epochs': 100,

    'local_epochs': 20,

    'local_lr': 0.01,

    'verbose_gap': 5,

    'dataset': 'mnist',

    'batch_size': -1,

    'gpu': 1,

    'log': 0,

    'seed': 17,

    'client_num_per_round': 5,

    'save_period': 5,

    'dp_sigma': 0.0,

    'clip_bound': 100.0

}

============================== TRAINING ==============================

============================== ROUND: 0 ==============================

client [3]   loss: 10.9515 -> 1.3190    accuracy: 0.00% -> 42.81%

client [4]   loss: 3.3307 -> 0.5750    accuracy: 26.63% -> 87.97%

client [0]   loss: 1.8386 -> 0.1959    accuracy: 46.55% -> 94.12%

client [1]   loss: 6.7608 -> 0.3468    accuracy: 16.33% -> 87.82%

client [2]   loss: 10.9939 -> 0.4377    accuracy: 1.06% -> 89.89%

============================== ROUND: 5 ==============================

client [4]   loss: 1.1147 -> 0.1165    accuracy: 64.37% -> 95.84%

client [0]   loss: 0.7093 -> 0.1016    accuracy: 84.65% -> 96.93%

client [1]   loss: 1.0910 -> 0.1722    accuracy: 65.17% -> 94.87%

client [2]   loss: 1.0483 -> 0.1281    accuracy: 68.56% -> 95.97%

client [3]   loss: 0.9943 -> 0.0626    accuracy: 70.09% -> 97.79%

============================== ROUND: 10 ==============================

client [4]   loss: 0.6783 -> 0.0877    accuracy: 78.05% -> 96.97%

client [1]   loss: 0.6435 -> 0.1365    accuracy: 78.34% -> 95.85%

client [0]   loss: 0.3042 -> 0.0610    accuracy: 91.56% -> 98.21%

client [2]   loss: 0.7089 -> 0.1075    accuracy: 76.75% -> 96.04%

client [3]   loss: 0.4690 -> 0.0456    accuracy: 85.20% -> 98.96%

============================== ROUND: 15 ==============================

client [1]   loss: 0.5336 -> 0.1187    accuracy: 84.13% -> 96.18%

client [4]   loss: 0.4952 -> 0.0748    accuracy: 82.62% -> 97.33%

client [0]   loss: 0.2061 -> 0.0496    accuracy: 93.61% -> 98.98%

client [2]   loss: 0.5523 -> 0.0699    accuracy: 82.50% -> 97.62%

client [3]   loss: 0.4038 -> 0.0449    accuracy: 86.98% -> 98.53%

============================== ROUND: 20 ==============================

client [2]   loss: 0.4201 -> 0.0581    accuracy: 86.79% -> 98.22%

client [0]   loss: 0.1916 -> 0.2922    accuracy: 94.88% -> 92.07%

client [1]   loss: 0.4241 -> 0.1057    accuracy: 87.56% -> 96.25%

client [4]   loss: 0.4819 -> 0.0687    accuracy: 82.31% -> 97.53%

client [3]   loss: 0.3464 -> 0.0359    accuracy: 89.56% -> 99.02%

============================== ROUND: 25 ==============================

client [4]   loss: 0.4533 -> 0.0612    accuracy: 83.08% -> 97.99%

client [0]   loss: 0.2029 -> 0.0421    accuracy: 94.12% -> 98.21%

client [1]   loss: 0.3930 -> 0.1056    accuracy: 88.48% -> 96.58%

client [3]   loss: 0.2955 -> 0.0341    accuracy: 91.09% -> 98.96%

client [2]   loss: 0.3682 -> 0.0493    accuracy: 88.38% -> 98.55%

============================== ROUND: 30 ==============================

client [3]   loss: 0.2879 -> 0.0326    accuracy: 91.52% -> 99.08%

client [1]   loss: 0.3488 -> 0.0949    accuracy: 89.86% -> 96.58%

client [0]   loss: 0.1829 -> 0.0470    accuracy: 94.88% -> 98.21%

client [4]   loss: 0.4008 -> 0.0556    accuracy: 85.66% -> 98.05%

client [2]   loss: 0.3752 -> 0.0461    accuracy: 88.57% -> 98.61%

============================== ROUND: 35 ==============================

client [0]   loss: 0.1670 -> 0.0644    accuracy: 95.14% -> 97.95%

client [3]   loss: 0.2693 -> 0.0273    accuracy: 91.83% -> 99.26%

client [2]   loss: 0.3469 -> 0.1385    accuracy: 89.10% -> 95.97%

client [1]   loss: 0.3431 -> 0.0854    accuracy: 90.59% -> 96.97%

Traceback (most recent call last):
  File "/home/local/ASURITE/xzhao181/DP_FedScaff_Stein/./src/server/fedavg.py", li
ne 24, in <module>
    server.run()
  File "/home/local/ASURITE/xzhao181/DP_FedScaff_Stein/src/server/base.py", line 2
11, in run
    self.train()
  File "/home/local/ASURITE/xzhao181/DP_FedScaff_Stein/src/server/base.py", line 1
12, in train
    res, stats, pseudo_grad = self.trainer.train(
  File "/home/local/ASURITE/xzhao181/DP_FedScaff_Stein/src/client/base.py", line 8
4, in train
    res, stats = self._log_while_training(evaluate, verbose, use_valset)()
  File "/home/local/ASURITE/xzhao181/DP_FedScaff_Stein/src/client/base.py", line 1
75, in _log_and_train

  File "/home/local/ASURITE/xzhao181/DP_FedScaff_Stein/src/client/base.py", line 1
16, in _train
    self.optimizer.step()
KeyboardInterrupt
Training... ━━━━━━━━━━━━━━╺━━━━━━━━━━━━━━━━━━━━━━━━━  35% 0:01:52

(scaffold) xzhao181@en4228312l:~/DP_FedScaff_Stein$ clear
(scaffold) xzhao181@en4228312l:~/DP_FedScaff_Stein$ python ./src/server/fedavg.py
--dataset mnist --clip_bound=100 --global_epochs=100 --local_epochs=20 --dp_sigma=
0
/home/local/ASURITE/xzhao181/DP_FedScaff_Stein/src/server/base.py:68: FutureWarnin
g: You are using `torch.load` with `weights_only=False` (the current default value
), which uses the default pickle module implicitly. It is possible to construct ma
licious pickle data which will execute arbitrary code during unpickling (See https
://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more deta
ils). In a future release, the default value for `weights_only` will be flipped to
 `True`. This limits the functions that could be executed during unpickling. Arbit
rary objects will no longer be allowed to be loaded via this mode unless they are
explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We
recommend you start setting `weights_only=True` for any use case where you don't h
ave full control of the loaded file. Please open an issue on GitHub for any issues
 related to this experimental feature.
  self.global_params_dict = torch.load(self.temp_dir / "global_model.pt")
Find existed global model...

Have run 30 epochs already.

Backbone: LeNet5(

  (net): Sequential(

    (0): Conv2d(1, 6, kernel_size=(5, 5), stride=(1, 1))

    (1): ReLU(inplace=True)

    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False
)
    (3): Conv2d(6, 16, kernel_size=(5, 5), stride=(1, 1))

    (4): ReLU(inplace=True)

    (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False
)
    (6): Flatten(start_dim=1, end_dim=-1)

    (7): Linear(in_features=256, out_features=120, bias=True)

    (8): ReLU(inplace=True)

    (9): Linear(in_features=120, out_features=84, bias=True)

    (10): ReLU(inplace=True)

    (11): Linear(in_features=84, out_features=10, bias=True)

  )

)

Arguments:

{

    'global_epochs': 100,

    'local_epochs': 20,

    'local_lr': 0.01,

    'verbose_gap': 5,

    'dataset': 'mnist',

    'batch_size': -1,

    'gpu': 1,

    'log': 0,

    'seed': 17,

    'client_num_per_round': 5,

    'save_period': 5,

    'dp_sigma': 0.0,

    'clip_bound': 100.0

}

============================== TRAINING ==============================

============================== ROUND: 0 ==============================

client [3]   loss: 0.2444 -> 0.0323    accuracy: 92.63% -> 98.96%

client [4]   loss: 0.3656 -> 0.0547    accuracy: 86.63% -> 98.30%

client [0]   loss: 0.1925 -> 0.0531    accuracy: 94.37% -> 97.70%

client [1]   loss: 0.3821 -> 0.0923    accuracy: 88.81% -> 96.71%

client [2]   loss: 0.3887 -> 0.0861    accuracy: 88.31% -> 97.49%

============================== ROUND: 5 ==============================

client [4]   loss: 0.3736 -> 0.0534    accuracy: 86.63% -> 98.30%

client [0]   loss: 0.1845 -> 0.0435    accuracy: 95.40% -> 98.72%

client [1]   loss: 0.3409 -> 0.0858    accuracy: 90.65% -> 96.64%

client [2]   loss: 0.3236 -> 0.0458    accuracy: 90.49% -> 98.61%

client [3]   loss: 0.2399 -> 0.0293    accuracy: 92.51% -> 99.08%

============================== ROUND: 10 ==============================

client [4]   loss: 0.3170 -> 0.0526    accuracy: 88.79% -> 98.41%

client [1]   loss: 0.3600 -> 0.0798    accuracy: 89.40% -> 97.10%

client [0]   loss: 0.1766 -> 0.0394    accuracy: 95.40% -> 98.72%

client [2]   loss: 0.3268 -> 0.0489    accuracy: 90.09% -> 98.35%

client [3]   loss: 0.2328 -> 0.0269    accuracy: 92.44% -> 99.20%

============================== ROUND: 15 ==============================

client [1]   loss: 0.2771 -> 0.0758    accuracy: 92.10% -> 97.04%

client [4]   loss: 0.3312 -> 0.0500    accuracy: 88.69% -> 98.30%

client [0]   loss: 0.1480 -> 0.0350    accuracy: 95.91% -> 99.74%

client [2]   loss: 0.3184 -> 0.0427    accuracy: 90.49% -> 98.75%

client [3]   loss: 0.2654 -> 0.0287    accuracy: 91.22% -> 99.20%

============================== ROUND: 20 ==============================

client [2]   loss: 0.2708 -> 0.0394    accuracy: 92.07% -> 98.88%

client [0]   loss: 0.1597 -> 0.0479    accuracy: 94.88% -> 97.19%

client [1]   loss: 0.2649 -> 0.0696    accuracy: 92.36% -> 97.63%

client [4]   loss: 0.3287 -> 0.0519    accuracy: 88.79% -> 98.25%

client [3]   loss: 0.2231 -> 0.0239    accuracy: 92.87% -> 99.32%

============================== ROUND: 25 ==============================

client [4]   loss: 0.2859 -> 0.0484    accuracy: 90.03% -> 98.66%

client [0]   loss: 0.1549 -> 0.0302    accuracy: 94.88% -> 98.98%

client [1]   loss: 0.2845 -> 0.0694    accuracy: 91.57% -> 97.50%

client [3]   loss: 0.2186 -> 0.0225    accuracy: 92.94% -> 99.32%

client [2]   loss: 0.2670 -> 0.0386    accuracy: 92.21% -> 99.01%

============================== ROUND: 30 ==============================

client [3]   loss: 0.2750 -> 0.0249    accuracy: 91.34% -> 99.32%

client [1]   loss: 0.2227 -> 0.0672    accuracy: 93.29% -> 97.56%

client [0]   loss: 0.1197 -> 0.0325    accuracy: 96.16% -> 98.47%

client [4]   loss: 0.2916 -> 0.0459    accuracy: 90.44% -> 98.41%

client [2]   loss: 0.3173 -> 0.0376    accuracy: 90.62% -> 98.94%

============================== ROUND: 35 ==============================

client [0]   loss: 0.1351 -> 0.0352    accuracy: 95.91% -> 99.23%

client [3]   loss: 0.2115 -> 0.0207    accuracy: 93.12% -> 99.51%

client [2]   loss: 0.2607 -> 0.0385    accuracy: 92.60% -> 98.94%

client [1]   loss: 0.2656 -> 0.0622    accuracy: 91.77% -> 97.89%

client [4]   loss: 0.2575 -> 0.0445    accuracy: 91.36% -> 98.66%

============================== ROUND: 40 ==============================

client [2]   loss: 0.2666 -> 0.0375    accuracy: 91.81% -> 98.94%

client [3]   loss: 0.2097 -> 0.0207    accuracy: 93.24% -> 99.57%

client [4]   loss: 0.2458 -> 0.0464    accuracy: 91.31% -> 98.51%

client [1]   loss: 0.2870 -> 0.0694    accuracy: 91.11% -> 97.37%

client [0]   loss: 0.1342 -> 0.0211    accuracy: 96.16% -> 98.98%

============================== ROUND: 45 ==============================

client [1]   loss: 0.2530 -> 0.0601    accuracy: 91.71% -> 97.83%

client [3]   loss: 0.2036 -> 0.0194    accuracy: 93.67% -> 99.45%

client [2]   loss: 0.2477 -> 0.0376    accuracy: 92.60% -> 99.01%

client [4]   loss: 0.2408 -> 0.0456    accuracy: 91.77% -> 98.61%

client [0]   loss: 0.1304 -> 0.0292    accuracy: 95.65% -> 99.23%

============================== ROUND: 50 ==============================

client [1]   loss: 0.2485 -> 0.0526    accuracy: 91.97% -> 98.16%

client [0]   loss: 0.1303 -> 0.0630    accuracy: 95.91% -> 97.95%

client [2]   loss: 0.2382 -> 0.0372    accuracy: 92.93% -> 99.14%

client [3]   loss: 0.1825 -> 0.0178    accuracy: 94.10% -> 99.57%

client [4]   loss: 0.2583 -> 0.0452    accuracy: 91.00% -> 98.61%

============================== ROUND: 55 ==============================

client [1]   loss: 0.2241 -> 0.0584    accuracy: 92.89% -> 97.96%

client [2]   loss: 0.2374 -> 0.0375    accuracy: 93.26% -> 98.94%

client [3]   loss: 0.1875 -> 0.0169    accuracy: 93.67% -> 99.63%

client [4]   loss: 0.2469 -> 0.0411    accuracy: 91.67% -> 98.66%

client [0]   loss: 0.1237 -> 0.0360    accuracy: 96.16% -> 98.98%

============================== ROUND: 60 ==============================

client [2]   loss: 0.2250 -> 0.0376    accuracy: 93.59% -> 99.08%

client [0]   loss: 0.1165 -> 0.0257    accuracy: 96.16% -> 98.98%

client [3]   loss: 0.1890 -> 0.0179    accuracy: 93.98% -> 99.51%

client [1]   loss: 0.2569 -> 0.0534    accuracy: 92.23% -> 98.49%

client [4]   loss: 0.2185 -> 0.0399    accuracy: 92.96% -> 98.82%

============================== ROUND: 65 ==============================

client [0]   loss: 0.1145 -> 0.0237    accuracy: 96.42% -> 98.98%

client [3]   loss: 0.1825 -> 0.0160    accuracy: 94.10% -> 99.57%

client [1]   loss: 0.2228 -> 0.0477    accuracy: 93.42% -> 98.29%

client [2]   loss: 0.2157 -> 0.0390    accuracy: 93.66% -> 99.01%

client [4]   loss: 0.2236 -> 0.0409    accuracy: 92.75% -> 98.77%

Training... ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 100% 0:01:58
============================== TESTING ==============================

Testing... ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 100% 0:00:01
==================== RESULTS ====================

loss: 0.1997    accuracy: 93.74%

FedAvg achieved 80.0% accuracy(89.30%) at epoch: 0

FedAvg achieved 90.0% accuracy(90.02%) at epoch: 3
                                      (scaffold) xzhao181@en4228312l:~/DP_FedScaff
(scaffold) xzhao181@en4228312l:~/DP_FedScaff_Stein$ tmux capture-pane -pS -1000 >
output.txt

