total_norm tensor(0.0431, device='cuda:0') self.clip_bound 100.0
total_norm tensor(0.0479, device='cuda:0') self.clip_bound 100.0
total_norm tensor(0.0526, device='cuda:0') self.clip_bound 100.0
total_norm tensor(0.0574, device='cuda:0') self.clip_bound 100.0
total_norm tensor(0.0620, device='cuda:0') self.clip_bound 100.0
total_norm tensor(0.0668, device='cuda:0') self.clip_bound 100.0
total_norm tensor(0.0716, device='cuda:0') self.clip_bound 100.0
total_norm tensor(0.0762, device='cuda:0') self.clip_bound 100.0
total_norm tensor(0.0809, device='cuda:0') self.clip_bound 100.0
total_norm tensor(0.0857, device='cuda:0') self.clip_bound 100.0
total_norm tensor(0.0904, device='cuda:0') self.clip_bound 100.0
total_norm tensor(0.0950, device='cuda:0') self.clip_bound 100.0
total_norm tensor(0.0052, device='cuda:0') self.clip_bound 100.0
total_norm tensor(0.0105, device='cuda:0') self.clip_bound 100.0
total_norm tensor(0.0156, device='cuda:0') self.clip_bound 100.0
total_norm tensor(0.0209, device='cuda:0') self.clip_bound 100.0
total_norm tensor(0.0262, device='cuda:0') self.clip_bound 100.0
total_norm tensor(0.0313, device='cuda:0') self.clip_bound 100.0
total_norm tensor(0.0364, device='cuda:0') self.clip_bound 100.0
total_norm tensor(0.0415, device='cuda:0') self.clip_bound 100.0
total_norm tensor(0.0468, device='cuda:0') self.clip_bound 100.0
total_norm tensor(0.0520, device='cuda:0') self.clip_bound 100.0
total_norm tensor(0.0572, device='cuda:0') self.clip_bound 100.0
total_norm tensor(0.0625, device='cuda:0') self.clip_bound 100.0
total_norm tensor(0.0677, device='cuda:0') self.clip_bound 100.0
total_norm tensor(0.0729, device='cuda:0') self.clip_bound 100.0
total_norm tensor(0.0781, device='cuda:0') self.clip_bound 100.0
total_norm tensor(0.0832, device='cuda:0') self.clip_bound 100.0
total_norm tensor(0.0884, device='cuda:0') self.clip_bound 100.0
total_norm tensor(0.0935, device='cuda:0') self.clip_bound 100.0
total_norm tensor(0.0986, device='cuda:0') self.clip_bound 100.0
total_norm tensor(0.1037, device='cuda:0') self.clip_bound 100.0
total_norm tensor(0.0060, device='cuda:0') self.clip_bound 100.0
total_norm tensor(0.0119, device='cuda:0') self.clip_bound 100.0
total_norm tensor(0.0178, device='cuda:0') self.clip_bound 100.0
total_norm tensor(0.0238, device='cuda:0') self.clip_bound 100.0
total_norm tensor(0.0297, device='cuda:0') self.clip_bound 100.0
total_norm tensor(0.0356, device='cuda:0') self.clip_bound 100.0
total_norm tensor(0.0415, device='cuda:0') self.clip_bound 100.0
total_norm tensor(0.0475, device='cuda:0') self.clip_bound 100.0
total_norm tensor(0.0534, device='cuda:0') self.clip_bound 100.0
total_norm tensor(0.0594, device='cuda:0') self.clip_bound 100.0
total_norm tensor(0.0654, device='cuda:0') self.clip_bound 100.0
total_norm tensor(0.0715, device='cuda:0') self.clip_bound 100.0
total_norm tensor(0.0774, device='cuda:0') self.clip_bound 100.0
total_norm tensor(0.0834, device='cuda:0') self.clip_bound 100.0
total_norm tensor(0.0893, device='cuda:0') self.clip_bound 100.0
total_norm tensor(0.0961, device='cuda:0') self.clip_bound 100.0
total_norm tensor(0.1020, device='cuda:0') self.clip_bound 100.0
total_norm tensor(0.1079, device='cuda:0') self.clip_bound 100.0
total_norm tensor(0.1138, device='cuda:0') self.clip_bound 100.0
total_norm tensor(0.1196, device='cuda:0') self.clip_bound 100.0
total_norm tensor(0.0063, device='cuda:0') self.clip_bound 100.0
total_norm tensor(0.0129, device='cuda:0') self.clip_bound 100.0
total_norm tensor(0.0199, device='cuda:0') self.clip_bound 100.0
total_norm tensor(0.0269, device='cuda:0') self.clip_bound 100.0
total_norm tensor(0.0334, device='cuda:0') self.clip_bound 100.0
total_norm tensor(0.0398, device='cuda:0') self.clip_bound 100.0
total_norm tensor(0.0465, device='cuda:0') self.clip_bound 100.0
total_norm tensor(0.0530, device='cuda:0') self.clip_bound 100.0
total_norm tensor(0.0797, device='cuda:0') self.clip_bound 100.0
total_norm tensor(0.0848, device='cuda:0') self.clip_bound 100.0
total_norm tensor(0.0902, device='cuda:0') self.clip_bound 100.0
total_norm tensor(0.0955, device='cuda:0') self.clip_bound 100.0
total_norm tensor(0.1009, device='cuda:0') self.clip_bound 100.0
total_norm tensor(0.1066, device='cuda:0') self.clip_bound 100.0
total_norm tensor(0.1126, device='cuda:0') self.clip_bound 100.0
total_norm tensor(0.1185, device='cuda:0') self.clip_bound 100.0
total_norm tensor(0.1246, device='cuda:0') self.clip_bound 100.0
total_norm tensor(0.1307, device='cuda:0') self.clip_bound 100.0
total_norm tensor(0.1362, device='cuda:0') self.clip_bound 100.0
total_norm tensor(0.1422, device='cuda:0') self.clip_bound 100.0
total_norm tensor(0.0060, device='cuda:0') self.clip_bound 100.0
total_norm tensor(0.0121, device='cuda:0') self.clip_bound 100.0
total_norm tensor(0.0181, device='cuda:0') self.clip_bound 100.0
total_norm tensor(0.0242, device='cuda:0') self.clip_bound 100.0
total_norm tensor(0.0301, device='cuda:0') self.clip_bound 100.0
total_norm tensor(0.0361, device='cuda:0') self.clip_bound 100.0
total_norm tensor(0.0421, device='cuda:0') self.clip_bound 100.0
total_norm tensor(0.0480, device='cuda:0') self.clip_bound 100.0
total_norm tensor(0.0540, device='cuda:0') self.clip_bound 100.0
total_norm tensor(0.0600, device='cuda:0') self.clip_bound 100.0
total_norm tensor(0.0659, device='cuda:0') self.clip_bound 100.0
total_norm tensor(0.0736, device='cuda:0') self.clip_bound 100.0
total_norm tensor(0.0811, device='cuda:0') self.clip_bound 100.0
total_norm tensor(0.0893, device='cuda:0') self.clip_bound 100.0
total_norm tensor(0.0949, device='cuda:0') self.clip_bound 100.0
total_norm tensor(0.1005, device='cuda:0') self.clip_bound 100.0
total_norm tensor(0.1061, device='cuda:0') self.clip_bound 100.0
total_norm tensor(0.1117, device='cuda:0') self.clip_bound 100.0
total_norm tensor(0.1173, device='cuda:0') self.clip_bound 100.0
total_norm tensor(0.1229, device='cuda:0') self.clip_bound 100.0
total_norm tensor(0.0054, device='cuda:0') self.clip_bound 100.0
total_norm tensor(0.0107, device='cuda:0') self.clip_bound 100.0
total_norm tensor(0.0161, device='cuda:0') self.clip_bound 100.0
total_norm tensor(0.0214, device='cuda:0') self.clip_bound 100.0
total_norm tensor(0.0268, device='cuda:0') self.clip_bound 100.0
total_norm tensor(0.0321, device='cuda:0') self.clip_bound 100.0
total_norm tensor(0.0374, device='cuda:0') self.clip_bound 100.0
total_norm tensor(0.0428, device='cuda:0') self.clip_bound 100.0
total_norm tensor(0.0481, device='cuda:0') self.clip_bound 100.0
total_norm tensor(0.0534, device='cuda:0') self.clip_bound 100.0
total_norm tensor(0.0587, device='cuda:0') self.clip_bound 100.0
total_norm tensor(0.0641, device='cuda:0') self.clip_bound 100.0
total_norm tensor(0.0694, device='cuda:0') self.clip_bound 100.0
total_norm tensor(0.0747, device='cuda:0') self.clip_bound 100.0
total_norm tensor(0.0799, device='cuda:0') self.clip_bound 100.0
total_norm tensor(0.0852, device='cuda:0') self.clip_bound 100.0
total_norm tensor(0.0905, device='cuda:0') self.clip_bound 100.0
total_norm tensor(0.0958, device='cuda:0') self.clip_bound 100.0
total_norm tensor(0.1011, device='cuda:0') self.clip_bound 100.0
total_norm tensor(0.1064, device='cuda:0') self.clip_bound 100.0
total_norm tensor(0.0053, device='cuda:0') self.clip_bound 100.0
total_norm tensor(0.0105, device='cuda:0') self.clip_bound 100.0
total_norm tensor(0.0158, device='cuda:0') self.clip_bound 100.0
total_norm tensor(0.0211, device='cuda:0') self.clip_bound 100.0
total_norm tensor(0.0261, device='cuda:0') self.clip_bound 100.0
total_norm tensor(0.0313, device='cuda:0') self.clip_bound 100.0
total_norm tensor(0.0364, device='cuda:0') self.clip_bound 100.0
total_norm tensor(0.0417, device='cuda:0') self.clip_bound 100.0
total_norm tensor(0.0470, device='cuda:0') self.clip_bound 100.0
total_norm tensor(0.0521, device='cuda:0') self.clip_bound 100.0
total_norm tensor(0.0574, device='cuda:0') self.clip_bound 100.0
total_norm tensor(0.0625, device='cuda:0') self.clip_bound 100.0
total_norm tensor(0.0677, device='cuda:0') self.clip_bound 100.0
total_norm tensor(0.0730, device='cuda:0') self.clip_bound 100.0
total_norm tensor(0.0781, device='cuda:0') self.clip_bound 100.0
total_norm tensor(0.0833, device='cuda:0') self.clip_bound 100.0
total_norm tensor(0.0884, device='cuda:0') self.clip_bound 100.0
total_norm tensor(0.0934, device='cuda:0') self.clip_bound 100.0
total_norm tensor(0.0985, device='cuda:0') self.clip_bound 100.0
total_norm tensor(0.1037, device='cuda:0') self.clip_bound 100.0
total_norm tensor(0.0049, device='cuda:0') self.clip_bound 100.0
total_norm tensor(0.0098, device='cuda:0') self.clip_bound 100.0
total_norm tensor(0.0147, device='cuda:0') self.clip_bound 100.0
total_norm tensor(0.0193, device='cuda:0') self.clip_bound 100.0
total_norm tensor(0.0241, device='cuda:0') self.clip_bound 100.0
total_norm tensor(0.0288, device='cuda:0') self.clip_bound 100.0
total_norm tensor(0.0336, device='cuda:0') self.clip_bound 100.0
total_norm tensor(0.0384, device='cuda:0') self.clip_bound 100.0
total_norm tensor(0.0432, device='cuda:0') self.clip_bound 100.0
total_norm tensor(0.0480, device='cuda:0') self.clip_bound 100.0
total_norm tensor(0.0528, device='cuda:0') self.clip_bound 100.0
total_norm tensor(0.0576, device='cuda:0') self.clip_bound 100.0
total_norm tensor(0.0623, device='cuda:0') self.clip_bound 100.0
total_norm tensor(0.0672, device='cuda:0') self.clip_bound 100.0
total_norm tensor(0.0720, device='cuda:0') self.clip_bound 100.0
total_norm tensor(0.0768, device='cuda:0') self.clip_bound 100.0
total_norm tensor(0.0815, device='cuda:0') self.clip_bound 100.0
total_norm tensor(0.0862, device='cuda:0') self.clip_bound 100.0
total_norm tensor(0.0909, device='cuda:0') self.clip_bound 100.0
total_norm tensor(0.0979, device='cuda:0') self.clip_bound 100.0
total_norm tensor(0.0060, device='cuda:0') self.clip_bound 100.0
total_norm tensor(0.0121, device='cuda:0') self.clip_bound 100.0
total_norm tensor(0.0243, device='cuda:0') self.clip_bound 100.0
total_norm tensor(0.0290, device='cuda:0') self.clip_bound 100.0
total_norm tensor(0.0341, device='cuda:0') self.clip_bound 100.0
total_norm tensor(0.0395, device='cuda:0') self.clip_bound 100.0
total_norm tensor(0.0450, device='cuda:0') self.clip_bound 100.0
total_norm tensor(0.0507, device='cuda:0') self.clip_bound 100.0
total_norm tensor(0.0563, device='cuda:0') self.clip_bound 100.0
total_norm tensor(0.0620, device='cuda:0') self.clip_bound 100.0
total_norm tensor(0.0677, device='cuda:0') self.clip_bound 100.0
total_norm tensor(0.0735, device='cuda:0') self.clip_bound 100.0
total_norm tensor(0.0793, device='cuda:0') self.clip_bound 100.0
total_norm tensor(0.0851, device='cuda:0') self.clip_bound 100.0
total_norm tensor(0.0909, device='cuda:0') self.clip_bound 100.0
total_norm tensor(0.0967, device='cuda:0') self.clip_bound 100.0
total_norm tensor(0.1025, device='cuda:0') self.clip_bound 100.0
total_norm tensor(0.1083, device='cuda:0') self.clip_bound 100.0
total_norm tensor(0.1142, device='cuda:0') self.clip_bound 100.0
total_norm tensor(0.1201, device='cuda:0') self.clip_bound 100.0
total_norm tensor(0.0054, device='cuda:0') self.clip_bound 100.0
total_norm tensor(0.0107, device='cuda:0') self.clip_bound 100.0
total_norm tensor(0.0160, device='cuda:0') self.clip_bound 100.0
total_norm tensor(0.0214, device='cuda:0') self.clip_bound 100.0
total_norm tensor(0.0268, device='cuda:0') self.clip_bound 100.0
total_norm tensor(0.0321, device='cuda:0') self.clip_bound 100.0
total_norm tensor(0.0374, device='cuda:0') self.clip_bound 100.0
total_norm tensor(0.0428, device='cuda:0') self.clip_bound 100.0
total_norm tensor(0.0481, device='cuda:0') self.clip_bound 100.0
total_norm tensor(0.0534, device='cuda:0') self.clip_bound 100.0
total_norm tensor(0.0587, device='cuda:0') self.clip_bound 100.0
total_norm tensor(0.0641, device='cuda:0') self.clip_bound 100.0
total_norm tensor(0.0693, device='cuda:0') self.clip_bound 100.0
total_norm tensor(0.0746, device='cuda:0') self.clip_bound 100.0
total_norm tensor(0.0799, device='cuda:0') self.clip_bound 100.0
total_norm tensor(0.0852, device='cuda:0') self.clip_bound 100.0
total_norm tensor(0.0904, device='cuda:0') self.clip_bound 100.0
total_norm tensor(0.0957, device='cuda:0') self.clip_bound 100.0
total_norm tensor(0.1009, device='cuda:0') self.clip_bound 100.0
total_norm tensor(0.1062, device='cuda:0') self.clip_bound 100.0
total_norm tensor(0.0053, device='cuda:0') self.clip_bound 100.0
total_norm tensor(0.0106, device='cuda:0') self.clip_bound 100.0
total_norm tensor(0.0159, device='cuda:0') self.clip_bound 100.0
total_norm tensor(0.0211, device='cuda:0') self.clip_bound 100.0
total_norm tensor(0.0263, device='cuda:0') self.clip_bound 100.0
total_norm tensor(0.0315, device='cuda:0') self.clip_bound 100.0
total_norm tensor(0.0367, device='cuda:0') self.clip_bound 100.0
total_norm tensor(0.0420, device='cuda:0') self.clip_bound 100.0
total_norm tensor(0.0474, device='cuda:0') self.clip_bound 100.0
total_norm tensor(0.0526, device='cuda:0') self.clip_bound 100.0
total_norm tensor(0.0577, device='cuda:0') self.clip_bound 100.0
total_norm tensor(0.0629, device='cuda:0') self.clip_bound 100.0
total_norm tensor(0.0680, device='cuda:0') self.clip_bound 100.0
total_norm tensor(0.0731, device='cuda:0') self.clip_bound 100.0
total_norm tensor(0.0782, device='cuda:0') self.clip_bound 100.0
total_norm tensor(0.0837, device='cuda:0') self.clip_bound 100.0
total_norm tensor(0.0888, device='cuda:0') self.clip_bound 100.0
total_norm tensor(0.0939, device='cuda:0') self.clip_bound 100.0
total_norm tensor(0.0992, device='cuda:0') self.clip_bound 100.0
total_norm tensor(0.1042, device='cuda:0') self.clip_bound 100.0
total_norm tensor(0.0049, device='cuda:0') self.clip_bound 100.0
total_norm tensor(0.0099, device='cuda:0') self.clip_bound 100.0
total_norm tensor(0.0148, device='cuda:0') self.clip_bound 100.0
total_norm tensor(0.0195, device='cuda:0') self.clip_bound 100.0
total_norm tensor(0.0241, device='cuda:0') self.clip_bound 100.0
total_norm tensor(0.0289, device='cuda:0') self.clip_bound 100.0
total_norm tensor(0.0337, device='cuda:0') self.clip_bound 100.0
total_norm tensor(0.0386, device='cuda:0') self.clip_bound 100.0
total_norm tensor(0.0434, device='cuda:0') self.clip_bound 100.0
total_norm tensor(0.0482, device='cuda:0') self.clip_bound 100.0
total_norm tensor(0.0529, device='cuda:0') self.clip_bound 100.0
total_norm tensor(0.0576, device='cuda:0') self.clip_bound 100.0
total_norm tensor(0.0624, device='cuda:0') self.clip_bound 100.0
total_norm tensor(0.0672, device='cuda:0') self.clip_bound 100.0
total_norm tensor(0.0720, device='cuda:0') self.clip_bound 100.0
total_norm tensor(0.0768, device='cuda:0') self.clip_bound 100.0
total_norm tensor(0.0815, device='cuda:0') self.clip_bound 100.0
total_norm tensor(0.0862, device='cuda:0') self.clip_bound 100.0
total_norm tensor(0.0908, device='cuda:0') self.clip_bound 100.0
total_norm tensor(0.0954, device='cuda:0') self.clip_bound 100.0
Traceback (most recent call last):
  File "/home/local/ASURITE/xzhao181/DP_FedScaff_Stein/./src/server/fedavg.py", line 24, in <module>
    server.run()
  File "/home/local/ASURITE/xzhao181/DP_FedScaff_Stein/src/server/base.py", line 211, in run
    self.train()
  File "/home/local/ASURITE/xzhao181/DP_FedScaff_Stein/src/server/base.py", line 112, in train
    res, stats, pseudo_grad = self.trainer.train(
  File "/home/local/ASURITE/xzhao181/DP_FedScaff_Stein/src/client/base.py", line 82, in train
    self.get_client_local_dataset()
  File "/home/local/ASURITE/xzhao181/DP_FedScaff_Stein/src/client/base.py", line 157, in get_client_local_dataset
    datasets = get_dataset(
  File "/home/local/ASURITE/xzhao181/DP_FedScaff_Stein/data/utils/util.py", line 36, in get_dataset
    subset = pickle.load(f)
  File "/home/local/ASURITE/xzhao181/anaconda3/envs/scaffold/lib/python3.9/site-packages/torch/storage.py", line 520, in _load_from_bytes
    return torch.load(io.BytesIO(b), weights_only=False)
  File "/home/local/ASURITE/xzhao181/anaconda3/envs/scaffold/lib/python3.9/site-packages/torch/serialization.py", line 1384, in load
    return _legacy_load(
  File "/home/local/ASURITE/xzhao181/anaconda3/envs/scaffold/lib/python3.9/site-packages/torch/serialization.py", line 1647, in _legacy_load
    typed_storage._untyped_storage._set_from_file(
KeyboardInterrupt
Training... ━━━━━━━━━━━━━━━━━━━━╸━━━━━━━━━━━━━━━━━━━  52% 0:01:19

(scaffold) xzhao181@en4228312l:~/DP_FedScaff_Stein$ clear
(scaffold) xzhao181@en4228312l:~/DP_FedScaff_Stein$ python ./src/server/fedavg.py --dataset mnist --clip_bound=100 --global_epochs=100 --local_epochs=20
 --dp_sigma=0.01
/home/local/ASURITE/xzhao181/DP_FedScaff_Stein/src/server/base.py:68: FutureWarning: You are using `torch.load` with `weights_only=False` (the current d
efault value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code duri
ng unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value f
or `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be all
owed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start se
tting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related t
o this experimental feature.
  self.global_params_dict = torch.load(self.temp_dir / "global_model.pt")
Find existed global model...
Have run 70 epochs already.
Backbone: LeNet5(
  (net): Sequential(
    (0): Conv2d(1, 6, kernel_size=(5, 5), stride=(1, 1))
    (1): ReLU(inplace=True)
    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
    (3): Conv2d(6, 16, kernel_size=(5, 5), stride=(1, 1))
    (4): ReLU(inplace=True)
    (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
    (6): Flatten(start_dim=1, end_dim=-1)
    (7): Linear(in_features=256, out_features=120, bias=True)
    (8): ReLU(inplace=True)
    (9): Linear(in_features=120, out_features=84, bias=True)
    (10): ReLU(inplace=True)
    (11): Linear(in_features=84, out_features=10, bias=True)
  )
)
Arguments:
{
    'global_epochs': 100,
    'local_epochs': 20,
    'local_lr': 0.01,
    'verbose_gap': 5,
    'dataset': 'mnist',
    'batch_size': -1,
    'gpu': 1,
    'log': 0,
    'seed': 17,
    'client_num_per_round': 5,
    'save_period': 5,
    'dp_sigma': 0.01,
    'clip_bound': 100.0
}
============================== TRAINING ==============================
============================== ROUND: 0 ==============================
client [3]   loss: 2.2331 -> 2.1794    accuracy: 42.69% -> 42.69%
client [4]   loss: 2.5348 -> 2.4794    accuracy: 0.05% -> 0.05%
client [0]   loss: 2.3336 -> 2.2481    accuracy: 0.00% -> 70.08%
client [1]   loss: 2.2993 -> 2.2533    accuracy: 0.00% -> 44.11%
client [2]   loss: 2.2301 -> 2.1585    accuracy: 0.00% -> 0.00%
============================== ROUND: 5 ==============================
client [4]   loss: 2.5258 -> 2.4707    accuracy: 0.05% -> 0.05%
client [0]   loss: 2.3409 -> 2.2537    accuracy: 0.00% -> 70.08%
client [1]   loss: 2.2442 -> 2.1984    accuracy: 0.00% -> 44.11%
client [2]   loss: 2.2276 -> 2.1561    accuracy: 0.00% -> 55.35%
client [3]   loss: 2.2352 -> 2.1818    accuracy: 42.69% -> 42.69%
============================== ROUND: 10 ==============================
client [4]   loss: 2.5095 -> 2.4550    accuracy: 0.05% -> 0.05%
client [1]   loss: 2.4200 -> 2.3739    accuracy: 0.00% -> 44.11%
client [0]   loss: 2.3320 -> 2.2455    accuracy: 0.00% -> 70.08%
client [2]   loss: 2.2382 -> 2.1619    accuracy: 0.00% -> 55.35%
client [3]   loss: 2.2392 -> 2.1859    accuracy: 42.69% -> 42.69%
============================== ROUND: 15 ==============================
client [1]   loss: 2.4484 -> 2.4018    accuracy: 0.00% -> 44.11%
client [4]   loss: 2.4972 -> 2.4431    accuracy: 0.05% -> 0.05%
client [0]   loss: 2.3339 -> 2.2463    accuracy: 0.00% -> 70.08%
client [2]   loss: 2.2267 -> 2.1632    accuracy: 0.00% -> 55.35%
client [3]   loss: 2.2427 -> 2.1887    accuracy: 42.69% -> 42.69%
============================== ROUND: 20 ==============================
client [2]   loss: 2.2258 -> 2.1546    accuracy: 0.00% -> 55.35%
client [0]   loss: 2.3338 -> 2.2471    accuracy: 0.00% -> 70.08%
client [1]   loss: 2.4651 -> 2.4178    accuracy: 0.00% -> 44.11%
client [4]   loss: 2.4782 -> 2.4249    accuracy: 0.05% -> 0.05%
client [3]   loss: 2.2429 -> 2.1892    accuracy: 42.69% -> 42.69%
Traceback (most recent call last):
  File "/home/local/ASURITE/xzhao181/DP_FedScaff_Stein/./src/server/fedavg.py", line 24, in <module>
    server.run()
  File "/home/local/ASURITE/xzhao181/DP_FedScaff_Stein/src/server/base.py", line 211, in run
    self.train()
  File "/home/local/ASURITE/xzhao181/DP_FedScaff_Stein/src/server/base.py", line 125, in train
    torch.save(
  File "/home/local/ASURITE/xzhao181/anaconda3/envs/scaffold/lib/python3.9/site-packages/torch/serialization.py", line 849, in save
    with _open_zipfile_writer(f) as opened_zipfile:
  File "/home/local/ASURITE/xzhao181/anaconda3/envs/scaffold/lib/python3.9/site-packages/torch/serialization.py", line 716, in _open_zipfile_writer
    return container(name_or_buffer)
  File "/home/local/ASURITE/xzhao181/anaconda3/envs/scaffold/lib/python3.9/site-packages/torch/serialization.py", line 687, in __init__
    super().__init__(torch._C.PyTorchFileWriter(self.name))
RuntimeError: Parent directory /home/local/ASURITE/xzhao181/DP_FedScaff_Stein/temp/FedAvg does not exist.
Training... ━━━━━━━━━━━━━━━━━━━━━━━━━━╸━━━━━━━━━━━━━  67% 0:00:21
(scaffold) xzhao181@en4228312l:~/DP_FedScaff_Stein$ clear
(scaffold) xzhao181@en4228312l:~/DP_FedScaff_Stein$ python ./src/server/fedavg.py --dataset mnist --clip_bound=100 --global_epochs=100 --local_epochs=20
 --dp_sigma=0.01
Backbone: LeNet5(
  (net): Sequential(
    (0): Conv2d(1, 6, kernel_size=(5, 5), stride=(1, 1))
    (1): ReLU(inplace=True)
    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
    (3): Conv2d(6, 16, kernel_size=(5, 5), stride=(1, 1))
    (4): ReLU(inplace=True)
    (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
    (6): Flatten(start_dim=1, end_dim=-1)
    (7): Linear(in_features=256, out_features=120, bias=True)
    (8): ReLU(inplace=True)
    (9): Linear(in_features=120, out_features=84, bias=True)
    (10): ReLU(inplace=True)
    (11): Linear(in_features=84, out_features=10, bias=True)
  )
)
Arguments:
{
    'global_epochs': 100,
    'local_epochs': 20,
    'local_lr': 0.01,
    'verbose_gap': 5,
    'dataset': 'mnist',
    'batch_size': -1,
    'gpu': 1,
    'log': 0,
    'seed': 17,
    'client_num_per_round': 5,
    'save_period': 5,
    'dp_sigma': 0.01,
    'clip_bound': 100.0
}
============================== TRAINING ==============================
============================== ROUND: 0 ==============================
client [3]   loss: 10.9515 -> 1.3190    accuracy: 0.00% -> 42.81%
client [4]   loss: 3.3307 -> 0.5750    accuracy: 26.63% -> 87.97%
client [0]   loss: 1.8386 -> 0.1959    accuracy: 46.55% -> 94.12%
client [1]   loss: 6.7608 -> 0.3468    accuracy: 16.33% -> 87.82%
client [2]   loss: 10.9939 -> 0.4377    accuracy: 1.06% -> 89.89%
============================== ROUND: 5 ==============================
client [4]   loss: 1.1271 -> 0.1192    accuracy: 62.83% -> 95.78%
client [0]   loss: 0.7016 -> 0.1276    accuracy: 83.12% -> 96.42%
client [1]   loss: 1.1249 -> 0.1835    accuracy: 62.48% -> 94.34%
client [2]   loss: 1.2831 -> 0.1341    accuracy: 60.37% -> 96.17%
client [3]   loss: 0.9005 -> 0.0611    accuracy: 75.25% -> 97.73%
============================== ROUND: 10 ==============================
client [4]   loss: 0.6032 -> 0.0997    accuracy: 80.51% -> 96.92%
client [1]   loss: 0.6641 -> 0.1394    accuracy: 77.49% -> 95.52%
client [0]   loss: 0.3043 -> 0.0753    accuracy: 90.54% -> 97.95%
client [2]   loss: 0.9392 -> 0.1094    accuracy: 68.36% -> 95.84%
client [3]   loss: 0.5544 -> 0.0485    accuracy: 81.51% -> 98.71%
============================== ROUND: 15 ==============================
client [1]   loss: 0.5183 -> 0.1234    accuracy: 85.25% -> 95.98%
client [4]   loss: 0.6903 -> 0.0875    accuracy: 75.99% -> 97.17%
client [0]   loss: 0.2499 -> 0.0455    accuracy: 92.58% -> 98.47%
client [2]   loss: 0.6400 -> 0.0845    accuracy: 80.65% -> 97.16%
client [3]   loss: 0.4200 -> 0.0433    accuracy: 85.87% -> 98.65%
============================== ROUND: 20 ==============================
client [2]   loss: 0.5666 -> 0.0792    accuracy: 82.56% -> 97.49%
client [0]   loss: 0.1916 -> 0.0446    accuracy: 94.12% -> 98.72%
client [1]   loss: 0.4818 -> 0.1087    accuracy: 84.40% -> 96.51%
client [4]   loss: 0.4814 -> 0.0801    accuracy: 83.60% -> 97.43%
client [3]   loss: 0.4981 -> 0.0444    accuracy: 83.23% -> 98.53%
============================== ROUND: 25 ==============================
client [4]   loss: 0.5033 -> 0.0774    accuracy: 83.44% -> 97.63%
client [0]   loss: 0.2002 -> 0.0371    accuracy: 93.09% -> 99.23%
client [1]   loss: 0.5355 -> 0.1121    accuracy: 84.27% -> 96.18%
client [3]   loss: 0.3076 -> 0.0427    accuracy: 89.07% -> 98.77%
client [2]   loss: 0.5552 -> 0.2265    accuracy: 82.96% -> 93.53%
============================== ROUND: 30 ==============================
client [3]   loss: 0.4490 -> 0.0383    accuracy: 86.06% -> 98.89%
client [1]   loss: 0.4135 -> 0.1039    accuracy: 87.29% -> 96.97%
client [0]   loss: 0.2031 -> 0.0411    accuracy: 93.86% -> 98.47%
client [4]   loss: 0.4875 -> 0.0639    accuracy: 83.55% -> 97.99%
client [2]   loss: 0.4107 -> 0.0658    accuracy: 86.66% -> 97.82%
============================== ROUND: 35 ==============================
client [0]   loss: 0.1635 -> 0.0419    accuracy: 94.37% -> 98.72%
client [3]   loss: 0.4467 -> 0.0418    accuracy: 85.50% -> 98.65%
client [2]   loss: 0.4715 -> 0.0640    accuracy: 85.40% -> 98.35%
client [1]   loss: 0.4160 -> 0.1017    accuracy: 86.83% -> 96.97%
client [4]   loss: 0.3953 -> 0.0609    accuracy: 86.53% -> 97.89%
============================== ROUND: 40 ==============================
client [2]   loss: 0.3918 -> 0.0594    accuracy: 87.05% -> 97.89%
client [3]   loss: 0.3102 -> 0.0421    accuracy: 89.62% -> 98.59%
client [4]   loss: 0.5438 -> 0.0579    accuracy: 81.18% -> 98.15%
client [1]   loss: 0.4140 -> 0.0989    accuracy: 87.10% -> 96.64%
client [0]   loss: 0.2465 -> 0.0471    accuracy: 91.82% -> 97.95%
============================== ROUND: 45 ==============================
client [1]   loss: 0.3993 -> 0.0851    accuracy: 87.62% -> 97.63%
client [3]   loss: 0.2878 -> 0.0390    accuracy: 91.28% -> 98.77%
client [2]   loss: 0.4539 -> 0.0539    accuracy: 86.20% -> 98.35%
client [4]   loss: 0.4404 -> 0.0532    accuracy: 84.73% -> 98.15%
client [0]   loss: 0.1625 -> 0.0527    accuracy: 94.12% -> 98.72%
============================== ROUND: 50 ==============================
client [1]   loss: 0.4545 -> 0.0785    accuracy: 84.86% -> 97.56%
client [0]   loss: 0.1651 -> 0.0603    accuracy: 94.63% -> 97.95%
client [2]   loss: 0.4196 -> 0.0577    accuracy: 87.19% -> 98.08%
client [3]   loss: 0.3296 -> 0.0346    accuracy: 89.31% -> 98.89%
client [4]   loss: 0.3868 -> 0.0533    accuracy: 87.15% -> 98.41%
============================== ROUND: 55 ==============================
client [1]   loss: 0.3109 -> 0.0796    accuracy: 91.11% -> 97.50%
client [2]   loss: 0.4261 -> 0.0560    accuracy: 86.53% -> 98.22%
client [3]   loss: 0.3181 -> 0.0334    accuracy: 90.42% -> 99.02%
client [4]   loss: 0.3722 -> 0.0429    accuracy: 87.46% -> 98.51%
client [0]   loss: 0.1562 -> 0.0483    accuracy: 96.16% -> 98.98%
============================== ROUND: 60 ==============================
client [2]   loss: 0.6055 -> 0.0594    accuracy: 81.77% -> 97.82%
client [0]   loss: 0.1590 -> 0.0339    accuracy: 95.91% -> 98.72%
client [3]   loss: 0.2802 -> 0.0347    accuracy: 91.58% -> 98.96%
client [1]   loss: 0.4257 -> 0.0859    accuracy: 86.37% -> 97.56%
client [4]   loss: 0.3056 -> 0.0449    accuracy: 89.87% -> 98.25%
============================== ROUND: 65 ==============================
client [0]   loss: 0.1413 -> 0.0300    accuracy: 95.40% -> 98.72%
client [3]   loss: 0.3199 -> 0.0344    accuracy: 90.05% -> 99.20%
client [1]   loss: 0.3557 -> 0.0656    accuracy: 89.27% -> 97.70%
client [2]   loss: 0.5205 -> 0.0520    accuracy: 85.14% -> 98.35%
client [4]   loss: 0.3099 -> 0.0412    accuracy: 90.03% -> 98.61%
============================== ROUND: 70 ==============================
client [3]   loss: 0.2386 -> 0.0323    accuracy: 92.81% -> 99.08%
client [0]   loss: 0.2057 -> 0.0334    accuracy: 93.86% -> 98.98%
client [4]   loss: 0.2713 -> 0.0534    accuracy: 90.90% -> 98.30%
client [1]   loss: 0.3558 -> 0.0631    accuracy: 89.01% -> 97.96%
client [2]   loss: 0.5661 -> 0.0526    accuracy: 83.88% -> 98.55%
============================== ROUND: 75 ==============================
client [3]   loss: 0.3587 -> 0.0330    accuracy: 89.50% -> 99.14%
client [4]   loss: 0.2486 -> 0.0433    accuracy: 91.93% -> 98.77%
client [1]   loss: 0.3220 -> 0.0532    accuracy: 89.27% -> 98.35%
client [0]   loss: 0.1013 -> 0.0384    accuracy: 96.42% -> 98.98%
client [2]   loss: 0.6559 -> 0.0584    accuracy: 81.64% -> 98.41%
============================== ROUND: 80 ==============================
client [3]   loss: 0.3398 -> 0.0333    accuracy: 90.36% -> 99.14%
client [1]   loss: 0.2962 -> 0.0536    accuracy: 91.31% -> 98.22%
client [2]   loss: 0.4356 -> 0.0540    accuracy: 87.91% -> 98.48%
client [0]   loss: 0.0965 -> 0.0299    accuracy: 96.68% -> 98.72%
client [4]   loss: 0.2842 -> 0.0451    accuracy: 90.64% -> 98.41%
============================== ROUND: 85 ==============================
client [3]   loss: 0.2801 -> 0.0346    accuracy: 91.28% -> 99.14%
client [2]   loss: 0.5912 -> 0.0572    accuracy: 83.88% -> 98.61%
client [0]   loss: 0.1122 -> 0.0355    accuracy: 95.91% -> 98.21%
client [4]   loss: 0.2810 -> 0.0411    accuracy: 90.39% -> 98.66%
client [1]   loss: 0.2563 -> 0.0588    accuracy: 92.50% -> 98.42%
============================== ROUND: 90 ==============================
client [4]   loss: 0.3419 -> 0.0411    accuracy: 88.43% -> 98.56%
client [2]   loss: 0.3586 -> 0.0558    accuracy: 90.22% -> 98.48%
client [1]   loss: 0.2549 -> 0.0530    accuracy: 92.36% -> 98.16%
client [0]   loss: 0.1423 -> 0.0611    accuracy: 95.91% -> 98.72%
client [3]   loss: 0.3093 -> 0.0285    accuracy: 90.29% -> 99.14%
============================== ROUND: 95 ==============================
client [2]   loss: 0.4208 -> 0.0602    accuracy: 88.24% -> 97.95%
client [3]   loss: 0.2323 -> 0.0263    accuracy: 92.20% -> 99.26%
client [0]   loss: 0.1670 -> 0.0244    accuracy: 95.40% -> 98.98%
client [1]   loss: 0.3022 -> 0.0616    accuracy: 91.44% -> 97.89%
client [4]   loss: 0.3425 -> 0.0446    accuracy: 89.00% -> 98.71%
Training... ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 100% 0:02:51
============================== TESTING ==============================
Testing... ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 100% 0:00:01
==================== RESULTS ====================
loss: 0.3291    accuracy: 90.30%
FedAvg achieved 10.0% accuracy(13.78%) at epoch: 0
FedAvg achieved 20.0% accuracy(21.04%) at epoch: 2
FedAvg achieved 40.0% accuracy(42.33%) at epoch: 3
FedAvg achieved 50.0% accuracy(53.17%) at epoch: 4
FedAvg achieved 60.0% accuracy(66.24%) at epoch: 5
FedAvg achieved 70.0% accuracy(73.66%) at epoch: 7
FedAvg achieved 80.0% accuracy(80.25%) at epoch: 11
FedAvg achieved 90.0% accuracy(90.47%) at epoch: 72
(scaffold) xzhao181@en4228312l:~/DP_FedScaff_Stein$ tmux capture-pane -pS -500 > results_g100_l20_fedavg_sigma0.1

