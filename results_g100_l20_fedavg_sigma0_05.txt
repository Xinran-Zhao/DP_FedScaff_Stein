total_norm tensor(0.0568, device='cuda:0') self.clip_bound 100.0
total_norm tensor(0.0619, device='cuda:0') self.clip_bound 100.0
total_norm tensor(0.0670, device='cuda:0') self.clip_bound 100.0
total_norm tensor(0.0721, device='cuda:0') self.clip_bound 100.0
total_norm tensor(0.0771, device='cuda:0') self.clip_bound 100.0
total_norm tensor(0.0822, device='cuda:0') self.clip_bound 100.0
total_norm tensor(0.0874, device='cuda:0') self.clip_bound 100.0
total_norm tensor(0.0926, device='cuda:0') self.clip_bound 100.0
total_norm tensor(0.0978, device='cuda:0') self.clip_bound 100.0
total_norm tensor(0.1028, device='cuda:0') self.clip_bound 100.0
Traceback (most recent call last):
  File "/home/local/ASURITE/xzhao181/DP_FedScaff_Stein/./src/server/fedavg.py", line 24, in <module>
    server.run()
  File "/home/local/ASURITE/xzhao181/DP_FedScaff_Stein/src/server/base.py", line 211, in run
    self.train()
  File "/home/local/ASURITE/xzhao181/DP_FedScaff_Stein/src/server/base.py", line 112, in train
    res, stats, pseudo_grad = self.trainer.train(
  File "/home/local/ASURITE/xzhao181/DP_FedScaff_Stein/src/client/base.py", line 82, in train
    self.get_client_local_dataset()
  File "/home/local/ASURITE/xzhao181/DP_FedScaff_Stein/src/client/base.py", line 157, in get_client_local_dataset
    datasets = get_dataset(
  File "/home/local/ASURITE/xzhao181/DP_FedScaff_Stein/data/utils/util.py", line 36, in get_dataset
    subset = pickle.load(f)
  File "/home/local/ASURITE/xzhao181/anaconda3/envs/scaffold/lib/python3.9/site-packages/torch/storage.py", line 519, in _load_from_byt
es
    def _load_from_bytes(b):
KeyboardInterrupt
Training... ━━━━━━━╺━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━  18% 0:01:39

(scaffold) xzhao181@en4228312l:~/DP_FedScaff_Stein$ clear
(scaffold) xzhao181@en4228312l:~/DP_FedScaff_Stein$ python ./src/server/fedavg.py --dataset mnist --clip_bound=100 --global_epochs=100
--local_epochs=20 --dp_sigma=0.05
/home/local/ASURITE/xzhao181/DP_FedScaff_Stein/src/server/base.py:68: FutureWarning: You are using `torch.load` with `weights_only=Fals
e` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data whic
h will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more
 details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be
 executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlis
ted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where y
ou don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  self.global_params_dict = torch.load(self.temp_dir / "global_model.pt")
Find existed global model...

Have run 75 epochs already.

Backbone: LeNet5(

  (net): Sequential(

    (0): Conv2d(1, 6, kernel_size=(5, 5), stride=(1, 1))

    (1): ReLU(inplace=True)

    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)

    (3): Conv2d(6, 16, kernel_size=(5, 5), stride=(1, 1))

    (4): ReLU(inplace=True)

    (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)

    (6): Flatten(start_dim=1, end_dim=-1)

    (7): Linear(in_features=256, out_features=120, bias=True)

    (8): ReLU(inplace=True)

    (9): Linear(in_features=120, out_features=84, bias=True)

    (10): ReLU(inplace=True)

    (11): Linear(in_features=84, out_features=10, bias=True)

  )

)

Arguments:

{

    'global_epochs': 100,

    'local_epochs': 20,

    'local_lr': 0.01,

    'verbose_gap': 5,

    'dataset': 'mnist',

    'batch_size': -1,

    'gpu': 1,

    'log': 0,

    'seed': 17,

    'client_num_per_round': 5,

    'save_period': 5,

    'dp_sigma': 0.05,

    'clip_bound': 100.0

}

============================== TRAINING ==============================

============================== ROUND: 0 ==============================

client [3]   loss: 2.2382 -> 2.1844    accuracy: 42.69% -> 42.69%

client [4]   loss: 2.5143 -> 2.4596    accuracy: 0.05% -> 0.05%

client [0]   loss: 2.3328 -> 2.2473    accuracy: 0.00% -> 70.08%

client [1]   loss: 2.3282 -> 2.2818    accuracy: 0.00% -> 44.11%

client [2]   loss: 2.2291 -> 2.1576    accuracy: 0.00% -> 55.35%

============================== ROUND: 5 ==============================

client [4]   loss: 2.5537 -> 2.4976    accuracy: 0.05% -> 0.05%

client [0]   loss: 2.3723 -> 2.2837    accuracy: 0.00% -> 0.00%

client [1]   loss: 2.1944 -> 2.1490    accuracy: 0.00% -> 44.11%

client [2]   loss: 2.2204 -> 2.1490    accuracy: 0.00% -> 0.00%

client [3]   loss: 2.2289 -> 2.1757    accuracy: 42.69% -> 42.69%

============================== ROUND: 10 ==============================

client [4]   loss: 2.5480 -> 2.4921    accuracy: 0.00% -> 0.00%

client [1]   loss: 2.1813 -> 2.1364    accuracy: 44.11% -> 44.11%

client [0]   loss: 2.3316 -> 2.2450    accuracy: 0.00% -> 0.00%

client [2]   loss: 2.2471 -> 2.1744    accuracy: 0.00% -> 0.00%

client [3]   loss: 2.2297 -> 2.1767    accuracy: 0.00% -> 42.69%

============================== ROUND: 15 ==============================

client [1]   loss: 2.2027 -> 2.1567    accuracy: 44.11% -> 44.11%

client [4]   loss: 2.5565 -> 2.5005    accuracy: 0.00% -> 0.00%

client [0]   loss: 2.3437 -> 2.2556    accuracy: 0.00% -> 70.08%

client [2]   loss: 2.2174 -> 2.1461    accuracy: 0.00% -> 55.35%

client [3]   loss: 2.2292 -> 2.1757    accuracy: 0.00% -> 42.69%

============================== ROUND: 20 ==============================

client [2]   loss: 2.2113 -> 2.1404    accuracy: 0.00% -> 0.00%

client [0]   loss: 2.3423 -> 2.2551    accuracy: 0.00% -> 0.00%

client [1]   loss: 2.2477 -> 2.1996    accuracy: 0.00% -> 0.00%

client [4]   loss: 2.5231 -> 2.4681    accuracy: 0.05% -> 0.05%

client [3]   loss: 2.2124 -> 2.1601    accuracy: 42.69% -> 42.69%

Traceback (most recent call last):
  File "/home/local/ASURITE/xzhao181/DP_FedScaff_Stein/./src/server/fedavg.py", line 24, in <module>
    server.run()
  File "/home/local/ASURITE/xzhao181/DP_FedScaff_Stein/src/server/base.py", line 211, in run
    self.train()
  File "/home/local/ASURITE/xzhao181/DP_FedScaff_Stein/src/server/base.py", line 125, in train
    torch.save(
  File "/home/local/ASURITE/xzhao181/anaconda3/envs/scaffold/lib/python3.9/site-packages/torch/serialization.py", line 849, in save
    with _open_zipfile_writer(f) as opened_zipfile:
  File "/home/local/ASURITE/xzhao181/anaconda3/envs/scaffold/lib/python3.9/site-packages/torch/serialization.py", line 716, in _open_zi
pfile_writer
    return container(name_or_buffer)
  File "/home/local/ASURITE/xzhao181/anaconda3/envs/scaffold/lib/python3.9/site-packages/torch/serialization.py", line 687, in __init__
    super().__init__(torch._C.PyTorchFileWriter(self.name))
RuntimeError: Parent directory /home/local/ASURITE/xzhao181/DP_FedScaff_Stein/temp/FedAvg does not exist.
Training... ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╺━━━━━━━  80% 0:00:10
(scaffold) xzhao181@en4228312l:~/DP_FedScaff_Stein$ clear
(scaffold) xzhao181@en4228312l:~/DP_FedScaff_Stein$ python ./src/server/fedavg.py --dataset mnist --clip_bound=100 --global_epochs=100
--local_epochs=20 --dp_sigma=0.05
Backbone: LeNet5(

  (net): Sequential(

    (0): Conv2d(1, 6, kernel_size=(5, 5), stride=(1, 1))

    (1): ReLU(inplace=True)

    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)

    (3): Conv2d(6, 16, kernel_size=(5, 5), stride=(1, 1))

    (4): ReLU(inplace=True)

    (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)

    (6): Flatten(start_dim=1, end_dim=-1)

    (7): Linear(in_features=256, out_features=120, bias=True)

    (8): ReLU(inplace=True)

    (9): Linear(in_features=120, out_features=84, bias=True)

    (10): ReLU(inplace=True)

    (11): Linear(in_features=84, out_features=10, bias=True)

  )

)

Arguments:

{

    'global_epochs': 100,

    'local_epochs': 20,

    'local_lr': 0.01,

    'verbose_gap': 5,

    'dataset': 'mnist',

    'batch_size': -1,

    'gpu': 1,

    'log': 0,

    'seed': 17,

    'client_num_per_round': 5,

    'save_period': 5,

    'dp_sigma': 0.05,

    'clip_bound': 100.0

}

============================== TRAINING ==============================

============================== ROUND: 0 ==============================

client [3]   loss: 10.9515 -> 1.3190    accuracy: 0.00% -> 42.81%

client [4]   loss: 3.3307 -> 0.5750    accuracy: 26.63% -> 87.97%

client [0]   loss: 1.8386 -> 0.1959    accuracy: 46.55% -> 94.12%

client [1]   loss: 6.7608 -> 0.3468    accuracy: 16.33% -> 87.82%

client [2]   loss: 10.9939 -> 0.4377    accuracy: 1.06% -> 89.89%

============================== ROUND: 5 ==============================

client [4]   loss: 2.1740 -> 0.1507    accuracy: 22.06% -> 95.12%

client [0]   loss: 2.1678 -> 0.1666    accuracy: 24.04% -> 95.40%

client [1]   loss: 2.7253 -> 0.2386    accuracy: 24.49% -> 92.50%

client [2]   loss: 2.9828 -> 0.1732    accuracy: 10.50% -> 94.12%

client [3]   loss: 1.7388 -> 0.1009    accuracy: 38.94% -> 96.38%

============================== ROUND: 10 ==============================

client [4]   loss: 1.8967 -> 0.1615    accuracy: 51.21% -> 95.37%

client [1]   loss: 2.7597 -> 0.2258    accuracy: 40.75% -> 93.02%

client [0]   loss: 1.4723 -> 0.1347    accuracy: 55.50% -> 96.68%

client [2]   loss: 2.8210 -> 0.2430    accuracy: 35.01% -> 92.54%

client [3]   loss: 3.5452 -> 0.1467    accuracy: 21.44% -> 95.82%

============================== ROUND: 15 ==============================

client [1]   loss: 2.4266 -> 0.5661    accuracy: 41.34% -> 75.51%

client [4]   loss: 2.2650 -> 0.2231    accuracy: 43.24% -> 93.06%

client [0]   loss: 1.4001 -> 0.2240    accuracy: 67.26% -> 93.35%

client [2]   loss: 2.8726 -> 0.2169    accuracy: 28.60% -> 93.20%

client [3]   loss: 1.1709 -> 0.1484    accuracy: 66.52% -> 95.33%

============================== ROUND: 20 ==============================

client [2]   loss: 2.5669 -> 0.2528    accuracy: 9.18% -> 91.68%

client [0]   loss: 2.0513 -> 0.3199    accuracy: 38.62% -> 90.79%

client [1]   loss: 1.9797 -> 0.3248    accuracy: 33.31% -> 88.61%

client [4]   loss: 2.1444 -> 0.2854    accuracy: 29.72% -> 92.13%

client [3]   loss: 2.4510 -> 0.1965    accuracy: 24.32% -> 94.53%

============================== ROUND: 25 ==============================

client [4]   loss: 3.0657 -> 0.3752    accuracy: 7.46% -> 86.58%

client [0]   loss: 2.1306 -> 0.5022    accuracy: 42.97% -> 83.63%

client [1]   loss: 1.6849 -> 0.5963    accuracy: 44.24% -> 76.83%

client [3]   loss: 2.3136 -> 0.1816    accuracy: 28.13% -> 94.41%

client [2]   loss: 1.6551 -> 0.2038    accuracy: 42.40% -> 94.45%

============================== ROUND: 30 ==============================

client [3]   loss: 2.2035 -> 0.3172    accuracy: 32.31% -> 91.15%

client [1]   loss: 1.5370 -> 0.5666    accuracy: 50.03% -> 78.34%

client [0]   loss: 1.4484 -> 0.3999    accuracy: 62.15% -> 86.70%

client [4]   loss: 2.4990 -> 0.4315    accuracy: 26.99% -> 85.19%

client [2]   loss: 1.1689 -> 0.1331    accuracy: 65.19% -> 96.10%

============================== ROUND: 35 ==============================

client [0]   loss: 1.3466 -> 0.3384    accuracy: 62.40% -> 88.49%

client [3]   loss: 1.7543 -> 0.2169    accuracy: 42.75% -> 93.30%

client [2]   loss: 1.1520 -> 0.1269    accuracy: 61.16% -> 96.24%

client [1]   loss: 1.6563 -> 0.5210    accuracy: 44.37% -> 83.15%

client [4]   loss: 1.6316 -> 0.3233    accuracy: 46.07% -> 87.97%

============================== ROUND: 40 ==============================

client [2]   loss: 2.5946 -> 0.1173    accuracy: 38.71% -> 96.43%

client [3]   loss: 1.0839 -> 0.2294    accuracy: 63.39% -> 92.20%

client [4]   loss: 1.7032 -> 0.2902    accuracy: 44.32% -> 89.77%

client [1]   loss: 2.1706 -> 0.3990    accuracy: 44.57% -> 84.40%

client [0]   loss: 0.9094 -> 0.2604    accuracy: 72.38% -> 92.07%

============================== ROUND: 45 ==============================

client [1]   loss: 1.5489 -> 0.6213    accuracy: 53.59% -> 77.81%

client [3]   loss: 1.3284 -> 0.2393    accuracy: 60.07% -> 92.14%

client [2]   loss: 2.0804 -> 0.1449    accuracy: 39.37% -> 95.97%

client [4]   loss: 1.3074 -> 0.2773    accuracy: 60.10% -> 91.11%

client [0]   loss: 0.9153 -> 0.2683    accuracy: 71.87% -> 92.07%

============================== ROUND: 50 ==============================

client [1]   loss: 2.4733 -> 0.4699    accuracy: 37.99% -> 79.66%

client [0]   loss: 0.8159 -> 0.2920    accuracy: 73.40% -> 91.30%

client [2]   loss: 3.0634 -> 0.1352    accuracy: 28.47% -> 96.17%

client [3]   loss: 2.0902 -> 0.2075    accuracy: 34.03% -> 93.12%

client [4]   loss: 1.1369 -> 0.2611    accuracy: 64.52% -> 91.16%

============================== ROUND: 55 ==============================

client [1]   loss: 1.6384 -> 0.4804    accuracy: 51.35% -> 80.32%

client [2]   loss: 1.4095 -> 0.1322    accuracy: 61.96% -> 96.43%

client [3]   loss: 1.8609 -> 0.2154    accuracy: 41.09% -> 93.37%

client [4]   loss: 1.3584 -> 0.2770    accuracy: 61.49% -> 91.52%

client [0]   loss: 0.9385 -> 0.2787    accuracy: 73.15% -> 90.79%

============================== ROUND: 60 ==============================

client [2]   loss: 2.7211 -> 0.1655    accuracy: 35.34% -> 95.90%

client [0]   loss: 1.3631 -> 0.2490    accuracy: 60.61% -> 92.84%

client [3]   loss: 1.6193 -> 0.1801    accuracy: 52.03% -> 94.66%

client [1]   loss: 1.7196 -> 0.3339    accuracy: 49.18% -> 86.64%

client [4]   loss: 1.5823 -> 0.2381    accuracy: 58.56% -> 92.03%

============================== ROUND: 65 ==============================

client [0]   loss: 1.3956 -> 0.2821    accuracy: 62.92% -> 91.30%

client [3]   loss: 2.1536 -> 0.1574    accuracy: 48.96% -> 95.02%

client [1]   loss: 1.1675 -> 0.4272    accuracy: 60.83% -> 82.36%

client [2]   loss: 2.7795 -> 0.1929    accuracy: 39.43% -> 95.24%

client [4]   loss: 1.3854 -> 0.2655    accuracy: 62.83% -> 91.77%

============================== ROUND: 70 ==============================

client [3]   loss: 2.0077 -> 0.2008    accuracy: 53.07% -> 93.98%

client [0]   loss: 1.3511 -> 0.2309    accuracy: 62.66% -> 91.56%

client [4]   loss: 1.0828 -> 0.2494    accuracy: 71.47% -> 92.54%

client [1]   loss: 1.7028 -> 0.4146    accuracy: 43.71% -> 84.60%

client [2]   loss: 2.7704 -> 0.2106    accuracy: 44.98% -> 94.58%

============================== ROUND: 75 ==============================

client [3]   loss: 1.4333 -> 0.2025    accuracy: 60.63% -> 93.98%

client [4]   loss: 2.5541 -> 0.2415    accuracy: 46.84% -> 92.44%

client [1]   loss: 1.7247 -> 0.4393    accuracy: 48.32% -> 85.52%

client [0]   loss: 1.2800 -> 0.2949    accuracy: 68.80% -> 91.82%

client [2]   loss: 2.1585 -> 0.1966    accuracy: 54.03% -> 94.85%

============================== ROUND: 80 ==============================

client [3]   loss: 3.6120 -> 0.1870    accuracy: 38.08% -> 94.84%

client [1]   loss: 1.7744 -> 0.4205    accuracy: 56.75% -> 83.28%

client [2]   loss: 3.6038 -> 0.2173    accuracy: 31.70% -> 95.31%

client [0]   loss: 1.3844 -> 0.2557    accuracy: 72.38% -> 92.07%

client [4]   loss: 2.7828 -> 0.2853    accuracy: 48.95% -> 91.05%

============================== ROUND: 85 ==============================

client [3]   loss: 1.6652 -> 0.2163    accuracy: 59.34% -> 94.59%

client [2]   loss: 2.6425 -> 0.2238    accuracy: 41.94% -> 94.98%

client [0]   loss: 1.0384 -> 0.2790    accuracy: 73.40% -> 92.07%

client [4]   loss: 2.1224 -> 0.3221    accuracy: 51.88% -> 90.75%

client [1]   loss: 1.7735 -> 0.4328    accuracy: 53.98% -> 82.82%

============================== ROUND: 90 ==============================

client [4]   loss: 2.2307 -> 0.3107    accuracy: 48.89% -> 91.26%

client [2]   loss: 3.0465 -> 0.2247    accuracy: 33.36% -> 94.91%

client [1]   loss: 1.0308 -> 0.3603    accuracy: 74.06% -> 87.03%

client [0]   loss: 1.3589 -> 0.3089    accuracy: 67.01% -> 89.77%

client [3]   loss: 1.7733 -> 0.2029    accuracy: 55.34% -> 94.23%

============================== ROUND: 95 ==============================

client [2]   loss: 1.9954 -> 0.2122    accuracy: 53.90% -> 95.38%

client [3]   loss: 2.4212 -> 0.2165    accuracy: 46.93% -> 93.86%

client [0]   loss: 0.9384 -> 0.3405    accuracy: 79.03% -> 91.82%

client [1]   loss: 2.4243 -> 0.3491    accuracy: 49.24% -> 87.29%

client [4]   loss: 1.4904 -> 0.2858    accuracy: 64.58% -> 91.16%

Training... ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 100% 0:02:51
============================== TESTING ==============================

Testing... ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 100% 0:00:01
==================== RESULTS ====================

loss: 2.3365    accuracy: 49.06%

FedAvg achieved 10.0% accuracy(13.78%) at epoch: 0

FedAvg achieved 20.0% accuracy(25.15%) at epoch: 2

FedAvg achieved 30.0% accuracy(32.60%) at epoch: 7

FedAvg achieved 40.0% accuracy(43.50%) at epoch: 11

FedAvg achieved 50.0% accuracy(51.58%) at epoch: 43

(scaffold) xzhao181@en4228312l:~/DP_FedScaff_Stein$ tmux capture-pane -pS -160 > results_g100_l20_fedavg_sigma0_05.txt
(scaffold) xzhao181@en4228312l:~/DP_FedScaff_Stein$ tmux capture-pane -pS -500 > results_g100_l20_fedavg_sigma0_05.txt

